{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'Target_ID': array(['Gene::1586', 'Gene::91', 'Gene::5291', ..., 'Gene::3782',\n",
      "       'Gene::8510', 'Gene::6330'], dtype=object), 'ProtBERT_Features': 0       [0.10895072, 0.07753971, 0.14122498, -0.004359...\n",
      "1       [0.07643488, 0.046613887, 0.11374605, 0.034627...\n",
      "2       [0.079430476, 0.028320394, 0.066061325, 0.0013...\n",
      "3       [0.13558188, 0.013845695, 0.12252106, 0.037885...\n",
      "4       [0.11597116, 0.016289648, 0.10930617, 0.023096...\n",
      "                              ...                        \n",
      "1380    [0.026317561, 0.002467662, 0.088176705, 0.0322...\n",
      "1381    [0.025081605, 0.011302358, 0.13916573, 0.03344...\n",
      "1382    [0.06859535, -0.012777425, 0.09564611, 0.01458...\n",
      "1383    [0.05675855, -0.017336804, 0.099945456, 0.0534...\n",
      "1384    [0.08563626, -0.021550683, 0.13873681, 0.03547...\n",
      "Name: ProtBERT_Features, Length: 1385, dtype: object, 'Gene_enco': array([1716., 1718., 1720., ...,   nan,   nan,   nan]), 'map_arr': array([   0,    1,    2, ..., 1382, 1383, 1384])}\n",
      "dict_keys(['Target_ID', 'ProtBERT_Features', 'Gene_enco', 'map_arr'])\n",
      "1385\n",
      "<class 'numpy.ndarray'>\n",
      "[1716. 1718. 1720. ...   nan   nan   nan]\n",
      "float64\n",
      "[                1716                 1718                 1720 ...\n",
      " -9223372036854775808 -9223372036854775808 -9223372036854775808]\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "with open('VEC_target_feats.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "print(type(x))\n",
    "print(x)\n",
    "print(x.keys())\n",
    "print(len(x['Gene_enco']))\n",
    "print(type(x['Gene_enco']))\n",
    "print(x['Gene_enco'])\n",
    "print(x['Gene_enco'].dtype)\n",
    "b = x['Gene_enco'].astype(int)\n",
    "print(b)\n",
    "print(b.dtype)\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def convert_to_nan(array, value):\n",
    "#     array = array.astype(np.float64)  # Convert the array to float64 to accommodate NaN\n",
    "#     array[array == value] = np.nan   # Replace the specific value with NaN\n",
    "#     return array\n",
    "\n",
    "# # Example usage:\n",
    "# input_array = np.array([1.5, 2.7, 3.2, -9223372036854775808, 4.9])\n",
    "# value_to_convert = -9223372036854775808\n",
    "\n",
    "# b = convert_to_nan(b, value_to_convert)\n",
    "# print(b)\n",
    "# print(b.dtype)\n",
    "\n",
    "# import pandas as pd\n",
    "# t = pd.DataFrame(x)\n",
    "# # print(t)\n",
    "# # t['Gene_enco'] = t['Gene_enco'].apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'Drug_ID': array(['DB00672', 'DB00116', 'DB06663', ..., 'DB00610', 'DB08903',\n",
      "       'DB00474'], dtype=object), 'Drug': array(['CCCNC(=O)NS(=O)(=O)C1=CC=C(C=C1)Cl',\n",
      "       'C1C(NC2=C(N1)N=C(NC2=O)N)CNC3=CC=C(C=C3)C(=O)N[C@@H](CCC(=O)O)C(=O)O',\n",
      "       'C1[C@H](CN2[C@@H]1C(=O)N[C@H](C(=O)N[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C2=O)CC3=CC=CC=C3)CC4=CC=C(C=C4)OCC5=CC=CC=C5)CCCCN)CC6=CNC7=CC=CC=C76)C8=CC=CC=C8)OC(=O)NCCN',\n",
      "       ..., 'C[C@@H]([C@@H](C1=CC(=CC=C1)O)O)N',\n",
      "       'CN(C)CC[C@@](C1=CC=CC2=CC=CC=C21)([C@H](C3=CC=CC=C3)C4=C(N=C5C=CC(=CC5=C4)Br)OC)O',\n",
      "       'CCC#CC(C)C1(C(=O)NC(=O)N(C1=O)C)CC=C'], dtype=object), 'Morgan_Features': 0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...\n",
      "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "6       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "                              ...                        \n",
      "9640    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9751    [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9760    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9787    [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9801    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: Morgan_Features, Length: 1478, dtype: object, 'Drug_enco': [569, nan, 1030, 668, 804, 231, 600, 228, 373, 164, 985, nan, 407, 1480, 234, 175, 166, 1080, nan, 580, 374, 677, 987, 712, 1633, 501, 280, 1049, 281, 482, 272, 1316, 509, 1634, 537, 967, 1078, nan, 270, 558, 443, 347, 779, 165, 468, 680, 823, 258, 1261, 450, 816, 1202, 1112, 460, 641, 532, 955, 586, 587, 1427, 813, 1154, 581, nan, 1568, nan, 198, 251, nan, 621, 645, 1019, 1578, 590, 608, 629, 31, 477, 992, 319, 326, 103, 1275, 332, 181, 143, 567, 976, 221, 294, 669, nan, 695, 530, 135, 156, 196, 709, 1260, 1201, 1196, 189, 202, 208, 664, 1118, 438, 414, 929, nan, 869, 937, 320, 266, 496, 467, 444, 458, 483, 857, 1020, 47, 515, 302, 200, 1306, 1058, 977, 536, 394, 1620, 740, 952, nan, 255, 884, 473, 619, 1299, 512, 1709, 516, nan, 1088, 193, 125, 684, 861, 1421, nan, 556, 161, 371, 235, 932, 598, 1304, 60, 750, 437, 1084, 1168, 59, 634, 1361, 1295, 681, 32, 704, 914, 80, 188, 620, 474, 1504, 448, 961, 354, 307, 593, 486, 44, 1006, 262, 648, 653, 76, 142, 343, 179, 585, 666, 1176, 472, 1254, 170, 1212, 316, 369, 404, 622, 194, 157, 1270, 92, 225, 748, 1156, 1481, 377, 676, 521, nan, 1138, nan, 308, 1437, 912, 1310, 1220, 523, 370, 882, 488, 315, 229, 53, 544, nan, 119, 260, 1251, 887, 158, 628, 1117, 1069, 871, 894, 1533, 724, 527, 1033, 1277, 1369, 659, 616, 1426, 820, 259, 39, 1278, 1428, 714, 1524, 1218, 18, 731, 886, 675, 63, 1133, 230, 275, 12, 1086, 691, 312, 264, 726, 982, 968, 405, nan, 1012, 916, 1091, 1208, 1256, 1348, 226, 565, 350, 577, 981, 267, 881, 415, 1131, 248, 959, 317, 499, 1157, 359, 487, 1585, 678, 273, 360, 1199, 299, 1219, 334, 392, 1169, 431, 661, 253, 757, 358, 203, 944, 357, 292, 503, 352, 1566, 570, 257, 201, nan, 533, 298, 926, nan, 764, 1537, 297, 1435, 540, 1335, 1098, 147, nan, 1170, 550, 440, 282, 172, 283, 1134, 26, nan, 171, 1081, 979, 207, nan, 574, 1470, 336, 839, 290, nan, 504, 836, 1455, 989, 805, 1367, 1197, 497, 526, 606, 364, 413, 162, 288, 1697, 428, 244, 233, 127, 22, 883, 694, 471, 177, 543, 983, nan, 35, 674, 834, 1036, 435, 199, 736, 1475, 755, 738, 1407, 618, 555, 721, 388, 739, 287, 673, 588, 252, 642, 1120, 1416, 1590, 1007, 276, 1328, 180, 159, 51, 79, 1401, nan, 1293, 1627, 106, nan, 1419, 416, 1115, 1243, 1695, 925, 713, 610, nan, nan, 785, 851, 1111, 1040, 1126, 1358, 67, 1282, 928, 1226, nan, 1085, 1207, 660, 69, 1074, 637, 457, 576, 500, 219, 939, 396, 70, 408, 566, 1354, 329, 1392, 153, 190, 247, 239, 859, 139, 98, 1271, 559, nan, 232, 330, 417, 1366, 454, 261, 1172, 730, 1123, 1001, 1094, 644, 120, nan, nan, 1323, 1430, 639, 1317, 1107, 439, 706, 592, 1011, 1648, 20, 187, 446, 495, 385, 274, 650, nan, 1287, 1456, 812, 1070, 1318, 335, 998, 209, 452, 786, 277, 400, 954, 1023, 1536, 409, 1184, 999, 700, nan, 1395, 346, 46, nan, 384, 124, 245, 831, 1389, 1341, 491, 885, nan, nan, 1060, 131, 1124, 1305, 455, 16, 601, 1145, 1359, 1093, 426, 1379, 447, 353, 640, 38, 866, 55, 901, 28, 484, 878, 1388, 379, 1614, 1400, 362, 72, 617, 571, 1408, 1026, 510, 442, nan, 611, 931, 563, 1301, 397, 1185, 41, 822, 578, 42, 195, 670, 478, 197, nan, 1279, 1671, 163, 663, 1281, 1343, 1373, 321, 430, 144, 1099, 1213, 137, 1032, 1018, 1376, nan, 1171, 83, 1386, 1229, 1378, 824, nan, 864, 529, 1233, 1531, 1532, 1037, 268, 561, 524, 507, 237, 531, 453, 1186, 705, 309, 1549, 671, 269, 940, 828, 465, 97, nan, 1474, 1095, nan, 214, 562, 632, 658, 656, 1189, 412, nan, 795, 327, 1615, nan, nan, 840, 652, 1268, 514, 459, 1097, nan, 1200, 224, 182, 278, nan, 1493, 907, 548, nan, 1402, 1068, 1272, 84, 434, 729, 818, 1090, 597, 57, 635, 568, 1136, nan, 322, 1410, nan, 1551, 82, 972, nan, nan, 1629, 609, 633, 490, 845, 698, 842, 522, 141, 542, 718, 1355, 686, 614, 324, 811, 1332, 1263, 420, 799, 1127, 1003, 1010, 575, 1646, 376, 43, 250, nan, 236, nan, 341, 1104, 206, 289, 1441, 451, 391, 1288, 505, 4, 549, nan, 525, 1028, 111, 295, nan, 1225, 424, 1653, 243, 1526, 1284, 1424, 1072, 238, 1031, nan, 1223, 210, 303, 1017, 655, nan, 715, 1631, 64, 1191, 1244, 1231, 1130, 30, 494, 121, 703, 429, 1022, 1173, 466, 1346, 492, 1494, 599, 1422, nan, 29, 21, 951, 1240, 604, 14, 508, nan, 973, 314, 909, 1298, 1655, 897, 375, 1057, 1079, 1433, 1055, nan, 1162, 1296, 541, 65, 651, 1014, 1308, 589, 410, 613, 892, 1210, 383, 432, 631, 602, 77, 1636, 774, 767, 835, 711, 689, 1309, nan, 749, 263, 546, 1110, nan, 934, nan, 36, 122, 806, 1067, 7, 1248, 1357, 481, 743, 1350, 1639, 964, 304, nan, 1224, 787, 1406, 390, 688, 1062, 560, 808, 1077, nan, 1675, 539, nan, nan, nan, 1265, 105, nan, 401, 56, 970, 310, 564, 1345, 647, 1663, 387, 1038, 441, 1352, 654, 68, 109, 1338, 1466, 183, 1016, 318, 1238, 34, 1507, 93, 1241, nan, 403, 138, 74, 518, nan, 1203, 880, 1477, 1076, 535, 991, 803, 249, 1071, 636, 1616, 1431, 1439, 978, 554, 506, 657, 223, 27, 1242, 296, 1021, 445, nan, nan, 1651, 284, 920, nan, 150, 1051, 345, 801, nan, 528, 513, 1362, 152, 623, 1542, nan, nan, 693, 285, 205, 915, 1015, 708, 572, 340, 1215, 867, nan, 485, nan, 662, 1425, 23, 114, 186, 980, 227, 1465, nan, 216, 265, 62, 1228, 1046, 386, 1365, 242, 717, 365, 933, 603, 1054, 1705, 311, 679, 1181, 50, 974, 596, 643, nan, 752, 699, 254, 630, 463, 363, 1574, 1383, 1486, nan, 1418, 889, 1283, 1517, 1190, 464, 502, 1525, 1065, 10, 81, 1562, 395, 1681, nan, 832, 852, 765, 1322, 54, 212, 1048, 1403, 798, 116, 1529, 1291, nan, 1472, 1511, 1042, 1230, 1029, nan, 423, 215, 99, 1039, 1409, nan, 192, 702, 1108, 1396, 1050, 461, nan, 1314, 1002, 638, 349, 775, 204, 1556, 2, nan, 313, 1412, nan, 672, 900, 1640, 582, 155, 128, 1385, 389, 1342, 140, 1125, 1660, 48, 1445, 40, 433, 1227, 1082, nan, 167, 1307, 517, 1482, 735, 1500, 1320, 971, 1478, 741, 1109, 519, 361, 151, 19, 1372, 425, 301, 351, 1259, 1143, 584, 191, 594, 753, 947, 579, 607, 1669, 919, 1334, 626, 1668, 291, 1027, 271, 328, 462, 176, 279, 1092, 893, 58, 727, 942, 846, 1035, nan, 96, 615, 300, 1484, 860, 1247, 11, nan, 1100, 720, 1178, 1234, 1161, 1187, 1547, 149, 1545, 1670, 145, 45, 1382, 355, 342, 1237, 1179, 965, 891, 211, 1641, 1654, 498, 745, 1121, 732, 1624, 797, nan, 1174, nan, 168, 101, 1688, 1488, 1083, 1321, nan, 911, 858, nan, 551, nan, 1129, nan, nan, 1327, nan, 762, 719, nan, 1297, 924, 1630, 1696, nan, nan, 1693, 1303, 830, 768, 1116, 1089, 338, 286, 534, 511, 1508, 1034, 722, 1004, 398, 850, 1368, 1515, 1045, 469, 1325, 761, 1609, 1520, 697, 844, 378, 1236, 331, 975, 411, 692, 957, 1180, 744, nan, 1375, 132, 344, 372, 1381, 862, 1292, 756, nan, 591, nan, 1114, 1005, 1499, 427, 1643, 1581, 723, 833, 1447, 476, 1135, 154, nan, nan, 1656, 173, 222, nan, 1394, 475, 1349, nan, 769, 73, 1680, 956, 1158, 1637, 17, 941, 218, 829, 493, 169, 1674, 1509, nan, nan, 117, 953, 849, nan, nan, nan, 115, 1538, 984, 1450, 1066, 784, 1257, 1464, 547, 305, 1175, 1324, 419, 1267, 649, nan, 1679, 754, nan, 545, 716, 1047, nan, nan, 1192, 479, 1420, nan, 113, 130, nan, nan, 1519, 994, 348, 104, 1141, 1290, nan, nan, 1177, nan, 123, 1132, 1044, 1502, 1326, 1052, 990, 1294, 1221, 1489, 1339, 333, nan, 792, 87, 380, nan, 1626, 1353, 1337, 1159, 710, 1621, 938, 1678, 1235, 966, 184, 1458, 1440, 1446, 75, 1252, nan, 107, 1591, 1521, 917, 782, 1087, 1209, 780, nan, 37, 1059, nan, 969, 174, 1061, 1404, nan, 1596, 1415, nan, 1103, nan, 1360, 1311, 1451, 1576, 24, nan, nan, 821, 421, 962, 1289, 489, 1113, 1604, 997, 1479, 1510, 1149, 945, 1552, 1473, 1652, 1105, 1587, 1053, 1102, 306, 874, 1364, nan, 1423, 809, 1483, 1000, 1214, 1398, 1347, 1393, 1468, 25, 133, 665, nan, 393, 1448, 682, 339, 557, 758, 1285, nan, 552, 1043, 1344, nan, 1024, 1650, 1232, nan, 1649, 110, 1041, 1672, 1013, 1457, 879, 1064, 1544, nan, 865, 725, 1535, 921, 1497, 1708, 751, 456, 1008, 800, 1258, 949, 583, nan, 890, 1550, 1211, 1165, 1255, 1262, 15, 1667, 1319, nan, 1595, 802, 788, 1063, 827, 1148, 1498, 734, 134, 1518, 1610, nan, 1249, 1530, nan, nan, 895, 1188, 728, 1315, 240]}\n",
      "dict_keys(['Drug_ID', 'Drug', 'Morgan_Features', 'Drug_enco'])\n",
      "1478\n",
      "<class 'list'>\n",
      "[569, nan, 1030, 668, 804, 231, 600, 228, 373, 164, 985, nan, 407, 1480, 234, 175, 166, 1080, nan, 580, 374, 677, 987, 712, 1633, 501, 280, 1049, 281, 482, 272, 1316, 509, 1634, 537, 967, 1078, nan, 270, 558, 443, 347, 779, 165, 468, 680, 823, 258, 1261, 450, 816, 1202, 1112, 460, 641, 532, 955, 586, 587, 1427, 813, 1154, 581, nan, 1568, nan, 198, 251, nan, 621, 645, 1019, 1578, 590, 608, 629, 31, 477, 992, 319, 326, 103, 1275, 332, 181, 143, 567, 976, 221, 294, 669, nan, 695, 530, 135, 156, 196, 709, 1260, 1201, 1196, 189, 202, 208, 664, 1118, 438, 414, 929, nan, 869, 937, 320, 266, 496, 467, 444, 458, 483, 857, 1020, 47, 515, 302, 200, 1306, 1058, 977, 536, 394, 1620, 740, 952, nan, 255, 884, 473, 619, 1299, 512, 1709, 516, nan, 1088, 193, 125, 684, 861, 1421, nan, 556, 161, 371, 235, 932, 598, 1304, 60, 750, 437, 1084, 1168, 59, 634, 1361, 1295, 681, 32, 704, 914, 80, 188, 620, 474, 1504, 448, 961, 354, 307, 593, 486, 44, 1006, 262, 648, 653, 76, 142, 343, 179, 585, 666, 1176, 472, 1254, 170, 1212, 316, 369, 404, 622, 194, 157, 1270, 92, 225, 748, 1156, 1481, 377, 676, 521, nan, 1138, nan, 308, 1437, 912, 1310, 1220, 523, 370, 882, 488, 315, 229, 53, 544, nan, 119, 260, 1251, 887, 158, 628, 1117, 1069, 871, 894, 1533, 724, 527, 1033, 1277, 1369, 659, 616, 1426, 820, 259, 39, 1278, 1428, 714, 1524, 1218, 18, 731, 886, 675, 63, 1133, 230, 275, 12, 1086, 691, 312, 264, 726, 982, 968, 405, nan, 1012, 916, 1091, 1208, 1256, 1348, 226, 565, 350, 577, 981, 267, 881, 415, 1131, 248, 959, 317, 499, 1157, 359, 487, 1585, 678, 273, 360, 1199, 299, 1219, 334, 392, 1169, 431, 661, 253, 757, 358, 203, 944, 357, 292, 503, 352, 1566, 570, 257, 201, nan, 533, 298, 926, nan, 764, 1537, 297, 1435, 540, 1335, 1098, 147, nan, 1170, 550, 440, 282, 172, 283, 1134, 26, nan, 171, 1081, 979, 207, nan, 574, 1470, 336, 839, 290, nan, 504, 836, 1455, 989, 805, 1367, 1197, 497, 526, 606, 364, 413, 162, 288, 1697, 428, 244, 233, 127, 22, 883, 694, 471, 177, 543, 983, nan, 35, 674, 834, 1036, 435, 199, 736, 1475, 755, 738, 1407, 618, 555, 721, 388, 739, 287, 673, 588, 252, 642, 1120, 1416, 1590, 1007, 276, 1328, 180, 159, 51, 79, 1401, nan, 1293, 1627, 106, nan, 1419, 416, 1115, 1243, 1695, 925, 713, 610, nan, nan, 785, 851, 1111, 1040, 1126, 1358, 67, 1282, 928, 1226, nan, 1085, 1207, 660, 69, 1074, 637, 457, 576, 500, 219, 939, 396, 70, 408, 566, 1354, 329, 1392, 153, 190, 247, 239, 859, 139, 98, 1271, 559, nan, 232, 330, 417, 1366, 454, 261, 1172, 730, 1123, 1001, 1094, 644, 120, nan, nan, 1323, 1430, 639, 1317, 1107, 439, 706, 592, 1011, 1648, 20, 187, 446, 495, 385, 274, 650, nan, 1287, 1456, 812, 1070, 1318, 335, 998, 209, 452, 786, 277, 400, 954, 1023, 1536, 409, 1184, 999, 700, nan, 1395, 346, 46, nan, 384, 124, 245, 831, 1389, 1341, 491, 885, nan, nan, 1060, 131, 1124, 1305, 455, 16, 601, 1145, 1359, 1093, 426, 1379, 447, 353, 640, 38, 866, 55, 901, 28, 484, 878, 1388, 379, 1614, 1400, 362, 72, 617, 571, 1408, 1026, 510, 442, nan, 611, 931, 563, 1301, 397, 1185, 41, 822, 578, 42, 195, 670, 478, 197, nan, 1279, 1671, 163, 663, 1281, 1343, 1373, 321, 430, 144, 1099, 1213, 137, 1032, 1018, 1376, nan, 1171, 83, 1386, 1229, 1378, 824, nan, 864, 529, 1233, 1531, 1532, 1037, 268, 561, 524, 507, 237, 531, 453, 1186, 705, 309, 1549, 671, 269, 940, 828, 465, 97, nan, 1474, 1095, nan, 214, 562, 632, 658, 656, 1189, 412, nan, 795, 327, 1615, nan, nan, 840, 652, 1268, 514, 459, 1097, nan, 1200, 224, 182, 278, nan, 1493, 907, 548, nan, 1402, 1068, 1272, 84, 434, 729, 818, 1090, 597, 57, 635, 568, 1136, nan, 322, 1410, nan, 1551, 82, 972, nan, nan, 1629, 609, 633, 490, 845, 698, 842, 522, 141, 542, 718, 1355, 686, 614, 324, 811, 1332, 1263, 420, 799, 1127, 1003, 1010, 575, 1646, 376, 43, 250, nan, 236, nan, 341, 1104, 206, 289, 1441, 451, 391, 1288, 505, 4, 549, nan, 525, 1028, 111, 295, nan, 1225, 424, 1653, 243, 1526, 1284, 1424, 1072, 238, 1031, nan, 1223, 210, 303, 1017, 655, nan, 715, 1631, 64, 1191, 1244, 1231, 1130, 30, 494, 121, 703, 429, 1022, 1173, 466, 1346, 492, 1494, 599, 1422, nan, 29, 21, 951, 1240, 604, 14, 508, nan, 973, 314, 909, 1298, 1655, 897, 375, 1057, 1079, 1433, 1055, nan, 1162, 1296, 541, 65, 651, 1014, 1308, 589, 410, 613, 892, 1210, 383, 432, 631, 602, 77, 1636, 774, 767, 835, 711, 689, 1309, nan, 749, 263, 546, 1110, nan, 934, nan, 36, 122, 806, 1067, 7, 1248, 1357, 481, 743, 1350, 1639, 964, 304, nan, 1224, 787, 1406, 390, 688, 1062, 560, 808, 1077, nan, 1675, 539, nan, nan, nan, 1265, 105, nan, 401, 56, 970, 310, 564, 1345, 647, 1663, 387, 1038, 441, 1352, 654, 68, 109, 1338, 1466, 183, 1016, 318, 1238, 34, 1507, 93, 1241, nan, 403, 138, 74, 518, nan, 1203, 880, 1477, 1076, 535, 991, 803, 249, 1071, 636, 1616, 1431, 1439, 978, 554, 506, 657, 223, 27, 1242, 296, 1021, 445, nan, nan, 1651, 284, 920, nan, 150, 1051, 345, 801, nan, 528, 513, 1362, 152, 623, 1542, nan, nan, 693, 285, 205, 915, 1015, 708, 572, 340, 1215, 867, nan, 485, nan, 662, 1425, 23, 114, 186, 980, 227, 1465, nan, 216, 265, 62, 1228, 1046, 386, 1365, 242, 717, 365, 933, 603, 1054, 1705, 311, 679, 1181, 50, 974, 596, 643, nan, 752, 699, 254, 630, 463, 363, 1574, 1383, 1486, nan, 1418, 889, 1283, 1517, 1190, 464, 502, 1525, 1065, 10, 81, 1562, 395, 1681, nan, 832, 852, 765, 1322, 54, 212, 1048, 1403, 798, 116, 1529, 1291, nan, 1472, 1511, 1042, 1230, 1029, nan, 423, 215, 99, 1039, 1409, nan, 192, 702, 1108, 1396, 1050, 461, nan, 1314, 1002, 638, 349, 775, 204, 1556, 2, nan, 313, 1412, nan, 672, 900, 1640, 582, 155, 128, 1385, 389, 1342, 140, 1125, 1660, 48, 1445, 40, 433, 1227, 1082, nan, 167, 1307, 517, 1482, 735, 1500, 1320, 971, 1478, 741, 1109, 519, 361, 151, 19, 1372, 425, 301, 351, 1259, 1143, 584, 191, 594, 753, 947, 579, 607, 1669, 919, 1334, 626, 1668, 291, 1027, 271, 328, 462, 176, 279, 1092, 893, 58, 727, 942, 846, 1035, nan, 96, 615, 300, 1484, 860, 1247, 11, nan, 1100, 720, 1178, 1234, 1161, 1187, 1547, 149, 1545, 1670, 145, 45, 1382, 355, 342, 1237, 1179, 965, 891, 211, 1641, 1654, 498, 745, 1121, 732, 1624, 797, nan, 1174, nan, 168, 101, 1688, 1488, 1083, 1321, nan, 911, 858, nan, 551, nan, 1129, nan, nan, 1327, nan, 762, 719, nan, 1297, 924, 1630, 1696, nan, nan, 1693, 1303, 830, 768, 1116, 1089, 338, 286, 534, 511, 1508, 1034, 722, 1004, 398, 850, 1368, 1515, 1045, 469, 1325, 761, 1609, 1520, 697, 844, 378, 1236, 331, 975, 411, 692, 957, 1180, 744, nan, 1375, 132, 344, 372, 1381, 862, 1292, 756, nan, 591, nan, 1114, 1005, 1499, 427, 1643, 1581, 723, 833, 1447, 476, 1135, 154, nan, nan, 1656, 173, 222, nan, 1394, 475, 1349, nan, 769, 73, 1680, 956, 1158, 1637, 17, 941, 218, 829, 493, 169, 1674, 1509, nan, nan, 117, 953, 849, nan, nan, nan, 115, 1538, 984, 1450, 1066, 784, 1257, 1464, 547, 305, 1175, 1324, 419, 1267, 649, nan, 1679, 754, nan, 545, 716, 1047, nan, nan, 1192, 479, 1420, nan, 113, 130, nan, nan, 1519, 994, 348, 104, 1141, 1290, nan, nan, 1177, nan, 123, 1132, 1044, 1502, 1326, 1052, 990, 1294, 1221, 1489, 1339, 333, nan, 792, 87, 380, nan, 1626, 1353, 1337, 1159, 710, 1621, 938, 1678, 1235, 966, 184, 1458, 1440, 1446, 75, 1252, nan, 107, 1591, 1521, 917, 782, 1087, 1209, 780, nan, 37, 1059, nan, 969, 174, 1061, 1404, nan, 1596, 1415, nan, 1103, nan, 1360, 1311, 1451, 1576, 24, nan, nan, 821, 421, 962, 1289, 489, 1113, 1604, 997, 1479, 1510, 1149, 945, 1552, 1473, 1652, 1105, 1587, 1053, 1102, 306, 874, 1364, nan, 1423, 809, 1483, 1000, 1214, 1398, 1347, 1393, 1468, 25, 133, 665, nan, 393, 1448, 682, 339, 557, 758, 1285, nan, 552, 1043, 1344, nan, 1024, 1650, 1232, nan, 1649, 110, 1041, 1672, 1013, 1457, 879, 1064, 1544, nan, 865, 725, 1535, 921, 1497, 1708, 751, 456, 1008, 800, 1258, 949, 583, nan, 890, 1550, 1211, 1165, 1255, 1262, 15, 1667, 1319, nan, 1595, 802, 788, 1063, 827, 1148, 1498, 734, 134, 1518, 1610, nan, 1249, 1530, nan, nan, 895, 1188, 728, 1315, 240]\n",
      "float64\n",
      "[                 569 -9223372036854775808                 1030 ...\n",
      "                  728                 1315                  240]\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "\n",
    "with open('VEC_drug_feats.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "print(type(x))\n",
    "print(x)\n",
    "print(x.keys())\n",
    "print(len(x['Drug_enco']))\n",
    "print(type(x['Drug_enco']))\n",
    "print(x['Drug_enco'])\n",
    "x['Drug_enco'] = np.array(x['Drug_enco'])\n",
    "print(x['Drug_enco'].dtype)\n",
    "b = x['Drug_enco'].astype(int)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### float to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Target_ID', 'Drug_ID', 'Target', 'Drug', 'Y', 'NCBI_ID', 'gene_enco',\n",
      "       'drug_enco'],\n",
      "      dtype='object')\n",
      "      gene_enco  drug_enco\n",
      "0       12069.0      187.0\n",
      "1       16051.0       70.0\n",
      "2       11671.0      255.0\n",
      "3        3134.0       41.0\n",
      "4        2460.0       67.0\n",
      "...         ...        ...\n",
      "1922     5524.0     1510.0\n",
      "1923    14649.0      735.0\n",
      "1924     2985.0      321.0\n",
      "1925     8339.0      529.0\n",
      "1926     7309.0      224.0\n",
      "\n",
      "[1927 rows x 2 columns]\n",
      "      gene_enco  drug_enco\n",
      "0         12069        187\n",
      "1         16051         70\n",
      "2         11671        255\n",
      "3          3134         41\n",
      "4          2460         67\n",
      "...         ...        ...\n",
      "1922       5524       1510\n",
      "1923      14649        735\n",
      "1924       2985        321\n",
      "1925       8339        529\n",
      "1926       7309        224\n",
      "\n",
      "[1927 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('vec_dev_origin.csv')\n",
    "print(df.columns)\n",
    "print(df[['gene_enco','drug_enco']])\n",
    "df['gene_enco'] = df['gene_enco'].copy().astype(int)\n",
    "df['drug_enco'] = df['drug_enco'].copy().astype(int)\n",
    "\n",
    "print(df[['gene_enco','drug_enco']])\n",
    "df.to_csv('vec_dev_origin.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data/vec/vec_dev_origin.csv에서 dev.txt에 있는 데이터만 vec_dev.csv로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gene_enco', 'drug_enco', 'Y'], dtype='object')\n",
      "      gene_enco  drug_enco  Y\n",
      "0          8945         74  0\n",
      "1          9832         10  0\n",
      "2          5178       1401  1\n",
      "3          6793       1325  1\n",
      "4          5586       1260  1\n",
      "...         ...        ... ..\n",
      "1774       4054       1196  0\n",
      "1775       7601       1018  0\n",
      "1776      12128        229  0\n",
      "1777      14697        694  1\n",
      "1778      10950        557  1\n",
      "\n",
      "[1779 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "vec_og = pd.read_csv('vec_dev_origin.csv', index_col=0)\n",
    "# print(vec_og.columns)\n",
    "# print(vec_og)\n",
    "\n",
    "vec = pd.read_csv('dev.txt', sep=' ',header=None)\n",
    "vec.columns = ['gene_enco','drug_enco',\"Y\"]\n",
    "print(vec.columns)\n",
    "print(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gene_enco  drug_enco  Y\n",
       "12128      1287       0    2\n",
       "4590       1282       0    2\n",
       "           1071       0    2\n",
       "18779      1627       0    2\n",
       "12128      1251       0    2\n",
       "                          ..\n",
       "7128       486        1    1\n",
       "           428        1    1\n",
       "           408        0    1\n",
       "           398        1    1\n",
       "18925      739        0    1\n",
       "Length: 1774, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    964\n",
      "0    963\n",
      "Name: Y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the vec_og DataFrame from 'vec_dev_origin.csv'\n",
    "vec_og = pd.read_csv('vec_dev_origin.csv', index_col=0)\n",
    "\n",
    "# Read the vec DataFrame from 'dev.txt'\n",
    "vec = pd.read_csv('dev.txt', sep=' ', header=None)\n",
    "vec.columns = ['gene_enco', 'drug_enco', 'Y']\n",
    "print(vec['Y'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Target_ID', 'ProtBERT_Features', 'Gene_enco', 'map_arr'])\n",
      "<class 'pandas.core.series.Series'>\n",
      "{'Target_ID': array(['Gene::1586', 'Gene::91', 'Gene::5291', ..., 'Gene::3782',\n",
      "       'Gene::8510', 'Gene::6330'], dtype=object), 'ProtBERT_Features': 0       [0.10895072, 0.07753971, 0.14122498, -0.004359...\n",
      "1       [0.07643488, 0.046613887, 0.11374605, 0.034627...\n",
      "2       [0.079430476, 0.028320394, 0.066061325, 0.0013...\n",
      "3       [0.13558188, 0.013845695, 0.12252106, 0.037885...\n",
      "4       [0.11597116, 0.016289648, 0.10930617, 0.023096...\n",
      "                              ...                        \n",
      "1380    [0.026317561, 0.002467662, 0.088176705, 0.0322...\n",
      "1381    [0.025081605, 0.011302358, 0.13916573, 0.03344...\n",
      "1382    [0.06859535, -0.012777425, 0.09564611, 0.01458...\n",
      "1383    [0.05675855, -0.017336804, 0.099945456, 0.0534...\n",
      "1384    [0.08563626, -0.021550683, 0.13873681, 0.03547...\n",
      "Name: ProtBERT_Features, Length: 1385, dtype: object, 'Gene_enco': array([1716., 1718., 1720., ...,   nan,   nan,   nan]), 'map_arr': array([   0,    1,    2, ..., 1382, 1383, 1384])}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('VEC_target_feats.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "print(x.keys())\n",
    "print(type((x['ProtBERT_Features'])))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Drug_ID', 'Drug', 'Morgan_Features', 'Drug_enco'])\n",
      "{'Drug_ID': array(['DB00672', 'DB00116', 'DB06663', ..., 'DB00610', 'DB08903',\n",
      "       'DB00474'], dtype=object), 'Drug': array(['CCCNC(=O)NS(=O)(=O)C1=CC=C(C=C1)Cl',\n",
      "       'C1C(NC2=C(N1)N=C(NC2=O)N)CNC3=CC=C(C=C3)C(=O)N[C@@H](CCC(=O)O)C(=O)O',\n",
      "       'C1[C@H](CN2[C@@H]1C(=O)N[C@H](C(=O)N[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C2=O)CC3=CC=CC=C3)CC4=CC=C(C=C4)OCC5=CC=CC=C5)CCCCN)CC6=CNC7=CC=CC=C76)C8=CC=CC=C8)OC(=O)NCCN',\n",
      "       ..., 'C[C@@H]([C@@H](C1=CC(=CC=C1)O)O)N',\n",
      "       'CN(C)CC[C@@](C1=CC=CC2=CC=CC=C21)([C@H](C3=CC=CC=C3)C4=C(N=C5C=CC(=CC5=C4)Br)OC)O',\n",
      "       'CCC#CC(C)C1(C(=O)NC(=O)N(C1=O)C)CC=C'], dtype=object), 'Morgan_Features': 0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...\n",
      "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "6       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "                              ...                        \n",
      "9640    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9751    [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9760    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9787    [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9801    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: Morgan_Features, Length: 1478, dtype: object, 'Drug_enco': [569, nan, 1030, 668, 804, 231, 600, 228, 373, 164, 985, nan, 407, 1480, 234, 175, 166, 1080, nan, 580, 374, 677, 987, 712, 1633, 501, 280, 1049, 281, 482, 272, 1316, 509, 1634, 537, 967, 1078, nan, 270, 558, 443, 347, 779, 165, 468, 680, 823, 258, 1261, 450, 816, 1202, 1112, 460, 641, 532, 955, 586, 587, 1427, 813, 1154, 581, nan, 1568, nan, 198, 251, nan, 621, 645, 1019, 1578, 590, 608, 629, 31, 477, 992, 319, 326, 103, 1275, 332, 181, 143, 567, 976, 221, 294, 669, nan, 695, 530, 135, 156, 196, 709, 1260, 1201, 1196, 189, 202, 208, 664, 1118, 438, 414, 929, nan, 869, 937, 320, 266, 496, 467, 444, 458, 483, 857, 1020, 47, 515, 302, 200, 1306, 1058, 977, 536, 394, 1620, 740, 952, nan, 255, 884, 473, 619, 1299, 512, 1709, 516, nan, 1088, 193, 125, 684, 861, 1421, nan, 556, 161, 371, 235, 932, 598, 1304, 60, 750, 437, 1084, 1168, 59, 634, 1361, 1295, 681, 32, 704, 914, 80, 188, 620, 474, 1504, 448, 961, 354, 307, 593, 486, 44, 1006, 262, 648, 653, 76, 142, 343, 179, 585, 666, 1176, 472, 1254, 170, 1212, 316, 369, 404, 622, 194, 157, 1270, 92, 225, 748, 1156, 1481, 377, 676, 521, nan, 1138, nan, 308, 1437, 912, 1310, 1220, 523, 370, 882, 488, 315, 229, 53, 544, nan, 119, 260, 1251, 887, 158, 628, 1117, 1069, 871, 894, 1533, 724, 527, 1033, 1277, 1369, 659, 616, 1426, 820, 259, 39, 1278, 1428, 714, 1524, 1218, 18, 731, 886, 675, 63, 1133, 230, 275, 12, 1086, 691, 312, 264, 726, 982, 968, 405, nan, 1012, 916, 1091, 1208, 1256, 1348, 226, 565, 350, 577, 981, 267, 881, 415, 1131, 248, 959, 317, 499, 1157, 359, 487, 1585, 678, 273, 360, 1199, 299, 1219, 334, 392, 1169, 431, 661, 253, 757, 358, 203, 944, 357, 292, 503, 352, 1566, 570, 257, 201, nan, 533, 298, 926, nan, 764, 1537, 297, 1435, 540, 1335, 1098, 147, nan, 1170, 550, 440, 282, 172, 283, 1134, 26, nan, 171, 1081, 979, 207, nan, 574, 1470, 336, 839, 290, nan, 504, 836, 1455, 989, 805, 1367, 1197, 497, 526, 606, 364, 413, 162, 288, 1697, 428, 244, 233, 127, 22, 883, 694, 471, 177, 543, 983, nan, 35, 674, 834, 1036, 435, 199, 736, 1475, 755, 738, 1407, 618, 555, 721, 388, 739, 287, 673, 588, 252, 642, 1120, 1416, 1590, 1007, 276, 1328, 180, 159, 51, 79, 1401, nan, 1293, 1627, 106, nan, 1419, 416, 1115, 1243, 1695, 925, 713, 610, nan, nan, 785, 851, 1111, 1040, 1126, 1358, 67, 1282, 928, 1226, nan, 1085, 1207, 660, 69, 1074, 637, 457, 576, 500, 219, 939, 396, 70, 408, 566, 1354, 329, 1392, 153, 190, 247, 239, 859, 139, 98, 1271, 559, nan, 232, 330, 417, 1366, 454, 261, 1172, 730, 1123, 1001, 1094, 644, 120, nan, nan, 1323, 1430, 639, 1317, 1107, 439, 706, 592, 1011, 1648, 20, 187, 446, 495, 385, 274, 650, nan, 1287, 1456, 812, 1070, 1318, 335, 998, 209, 452, 786, 277, 400, 954, 1023, 1536, 409, 1184, 999, 700, nan, 1395, 346, 46, nan, 384, 124, 245, 831, 1389, 1341, 491, 885, nan, nan, 1060, 131, 1124, 1305, 455, 16, 601, 1145, 1359, 1093, 426, 1379, 447, 353, 640, 38, 866, 55, 901, 28, 484, 878, 1388, 379, 1614, 1400, 362, 72, 617, 571, 1408, 1026, 510, 442, nan, 611, 931, 563, 1301, 397, 1185, 41, 822, 578, 42, 195, 670, 478, 197, nan, 1279, 1671, 163, 663, 1281, 1343, 1373, 321, 430, 144, 1099, 1213, 137, 1032, 1018, 1376, nan, 1171, 83, 1386, 1229, 1378, 824, nan, 864, 529, 1233, 1531, 1532, 1037, 268, 561, 524, 507, 237, 531, 453, 1186, 705, 309, 1549, 671, 269, 940, 828, 465, 97, nan, 1474, 1095, nan, 214, 562, 632, 658, 656, 1189, 412, nan, 795, 327, 1615, nan, nan, 840, 652, 1268, 514, 459, 1097, nan, 1200, 224, 182, 278, nan, 1493, 907, 548, nan, 1402, 1068, 1272, 84, 434, 729, 818, 1090, 597, 57, 635, 568, 1136, nan, 322, 1410, nan, 1551, 82, 972, nan, nan, 1629, 609, 633, 490, 845, 698, 842, 522, 141, 542, 718, 1355, 686, 614, 324, 811, 1332, 1263, 420, 799, 1127, 1003, 1010, 575, 1646, 376, 43, 250, nan, 236, nan, 341, 1104, 206, 289, 1441, 451, 391, 1288, 505, 4, 549, nan, 525, 1028, 111, 295, nan, 1225, 424, 1653, 243, 1526, 1284, 1424, 1072, 238, 1031, nan, 1223, 210, 303, 1017, 655, nan, 715, 1631, 64, 1191, 1244, 1231, 1130, 30, 494, 121, 703, 429, 1022, 1173, 466, 1346, 492, 1494, 599, 1422, nan, 29, 21, 951, 1240, 604, 14, 508, nan, 973, 314, 909, 1298, 1655, 897, 375, 1057, 1079, 1433, 1055, nan, 1162, 1296, 541, 65, 651, 1014, 1308, 589, 410, 613, 892, 1210, 383, 432, 631, 602, 77, 1636, 774, 767, 835, 711, 689, 1309, nan, 749, 263, 546, 1110, nan, 934, nan, 36, 122, 806, 1067, 7, 1248, 1357, 481, 743, 1350, 1639, 964, 304, nan, 1224, 787, 1406, 390, 688, 1062, 560, 808, 1077, nan, 1675, 539, nan, nan, nan, 1265, 105, nan, 401, 56, 970, 310, 564, 1345, 647, 1663, 387, 1038, 441, 1352, 654, 68, 109, 1338, 1466, 183, 1016, 318, 1238, 34, 1507, 93, 1241, nan, 403, 138, 74, 518, nan, 1203, 880, 1477, 1076, 535, 991, 803, 249, 1071, 636, 1616, 1431, 1439, 978, 554, 506, 657, 223, 27, 1242, 296, 1021, 445, nan, nan, 1651, 284, 920, nan, 150, 1051, 345, 801, nan, 528, 513, 1362, 152, 623, 1542, nan, nan, 693, 285, 205, 915, 1015, 708, 572, 340, 1215, 867, nan, 485, nan, 662, 1425, 23, 114, 186, 980, 227, 1465, nan, 216, 265, 62, 1228, 1046, 386, 1365, 242, 717, 365, 933, 603, 1054, 1705, 311, 679, 1181, 50, 974, 596, 643, nan, 752, 699, 254, 630, 463, 363, 1574, 1383, 1486, nan, 1418, 889, 1283, 1517, 1190, 464, 502, 1525, 1065, 10, 81, 1562, 395, 1681, nan, 832, 852, 765, 1322, 54, 212, 1048, 1403, 798, 116, 1529, 1291, nan, 1472, 1511, 1042, 1230, 1029, nan, 423, 215, 99, 1039, 1409, nan, 192, 702, 1108, 1396, 1050, 461, nan, 1314, 1002, 638, 349, 775, 204, 1556, 2, nan, 313, 1412, nan, 672, 900, 1640, 582, 155, 128, 1385, 389, 1342, 140, 1125, 1660, 48, 1445, 40, 433, 1227, 1082, nan, 167, 1307, 517, 1482, 735, 1500, 1320, 971, 1478, 741, 1109, 519, 361, 151, 19, 1372, 425, 301, 351, 1259, 1143, 584, 191, 594, 753, 947, 579, 607, 1669, 919, 1334, 626, 1668, 291, 1027, 271, 328, 462, 176, 279, 1092, 893, 58, 727, 942, 846, 1035, nan, 96, 615, 300, 1484, 860, 1247, 11, nan, 1100, 720, 1178, 1234, 1161, 1187, 1547, 149, 1545, 1670, 145, 45, 1382, 355, 342, 1237, 1179, 965, 891, 211, 1641, 1654, 498, 745, 1121, 732, 1624, 797, nan, 1174, nan, 168, 101, 1688, 1488, 1083, 1321, nan, 911, 858, nan, 551, nan, 1129, nan, nan, 1327, nan, 762, 719, nan, 1297, 924, 1630, 1696, nan, nan, 1693, 1303, 830, 768, 1116, 1089, 338, 286, 534, 511, 1508, 1034, 722, 1004, 398, 850, 1368, 1515, 1045, 469, 1325, 761, 1609, 1520, 697, 844, 378, 1236, 331, 975, 411, 692, 957, 1180, 744, nan, 1375, 132, 344, 372, 1381, 862, 1292, 756, nan, 591, nan, 1114, 1005, 1499, 427, 1643, 1581, 723, 833, 1447, 476, 1135, 154, nan, nan, 1656, 173, 222, nan, 1394, 475, 1349, nan, 769, 73, 1680, 956, 1158, 1637, 17, 941, 218, 829, 493, 169, 1674, 1509, nan, nan, 117, 953, 849, nan, nan, nan, 115, 1538, 984, 1450, 1066, 784, 1257, 1464, 547, 305, 1175, 1324, 419, 1267, 649, nan, 1679, 754, nan, 545, 716, 1047, nan, nan, 1192, 479, 1420, nan, 113, 130, nan, nan, 1519, 994, 348, 104, 1141, 1290, nan, nan, 1177, nan, 123, 1132, 1044, 1502, 1326, 1052, 990, 1294, 1221, 1489, 1339, 333, nan, 792, 87, 380, nan, 1626, 1353, 1337, 1159, 710, 1621, 938, 1678, 1235, 966, 184, 1458, 1440, 1446, 75, 1252, nan, 107, 1591, 1521, 917, 782, 1087, 1209, 780, nan, 37, 1059, nan, 969, 174, 1061, 1404, nan, 1596, 1415, nan, 1103, nan, 1360, 1311, 1451, 1576, 24, nan, nan, 821, 421, 962, 1289, 489, 1113, 1604, 997, 1479, 1510, 1149, 945, 1552, 1473, 1652, 1105, 1587, 1053, 1102, 306, 874, 1364, nan, 1423, 809, 1483, 1000, 1214, 1398, 1347, 1393, 1468, 25, 133, 665, nan, 393, 1448, 682, 339, 557, 758, 1285, nan, 552, 1043, 1344, nan, 1024, 1650, 1232, nan, 1649, 110, 1041, 1672, 1013, 1457, 879, 1064, 1544, nan, 865, 725, 1535, 921, 1497, 1708, 751, 456, 1008, 800, 1258, 949, 583, nan, 890, 1550, 1211, 1165, 1255, 1262, 15, 1667, 1319, nan, 1595, 802, 788, 1063, 827, 1148, 1498, 734, 134, 1518, 1610, nan, 1249, 1530, nan, nan, 895, 1188, 728, 1315, 240]}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('VEC_drug_feats.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "print(x.keys())\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋별 클래스 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    6183\n",
      "1    6148\n",
      "Name: 2, dtype: int64\n",
      "1    964\n",
      "0    963\n",
      "Name: 2, dtype: int64\n",
      "1    1779\n",
      "0    1745\n",
      "Name: 2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('train.txt', sep=' ', header=None)\n",
    "val = pd.read_csv('dev.txt', sep=' ', header=None)\n",
    "test = pd.read_csv('test.txt', sep=' ', header=None)\n",
    "\n",
    "print(train[2].value_counts())\n",
    "print(val[2].value_counts())\n",
    "print(test[2].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## protT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/ProstT5 were not used when initializing T5EncoderModel: ['decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9, 1024])\n",
      "tensor([[[-1.7776e-02, -9.8267e-03, -2.6428e-02,  ...,  3.0807e-02,\n",
      "          -8.5831e-03, -1.2054e-02],\n",
      "         [-1.1469e-01, -6.5796e-02,  5.4626e-02,  ...,  3.8574e-01,\n",
      "          -8.4961e-02,  4.3976e-02],\n",
      "         [-1.4734e-01, -2.0776e-01, -4.3042e-01,  ...,  3.9648e-01,\n",
      "          -1.1389e-01, -7.5073e-02],\n",
      "         ...,\n",
      "         [-2.5903e-01, -2.9434e-02,  3.0823e-02,  ...,  5.3986e-02,\n",
      "           4.7485e-02, -6.8665e-02],\n",
      "         [ 6.6223e-02,  1.3176e-02,  3.8574e-02,  ...,  1.2268e-01,\n",
      "          -6.2195e-02, -1.1792e-01],\n",
      "         [ 3.4809e-03, -6.4774e-03,  6.7329e-03,  ...,  3.4668e-02,\n",
      "           1.7563e-02,  5.0446e-02]],\n",
      "\n",
      "        [[-1.7776e-02, -9.8267e-03, -2.6428e-02,  ...,  3.0807e-02,\n",
      "          -8.5831e-03, -1.2054e-02],\n",
      "         [-1.1469e-01, -6.5796e-02,  5.4626e-02,  ...,  3.8574e-01,\n",
      "          -8.4961e-02,  4.3976e-02],\n",
      "         [-1.4734e-01, -2.0776e-01, -4.3042e-01,  ...,  3.9648e-01,\n",
      "          -1.1389e-01, -7.5073e-02],\n",
      "         ...,\n",
      "         [-2.5903e-01, -2.9434e-02,  3.0823e-02,  ...,  5.3986e-02,\n",
      "           4.7485e-02, -6.8665e-02],\n",
      "         [ 6.6223e-02,  1.3176e-02,  3.8574e-02,  ...,  1.2268e-01,\n",
      "          -6.2195e-02, -1.1792e-01],\n",
      "         [ 3.4809e-03, -6.4774e-03,  6.7329e-03,  ...,  3.4668e-02,\n",
      "           1.7563e-02,  5.0446e-02]],\n",
      "\n",
      "        [[ 8.1482e-02, -5.7251e-02, -4.9622e-02,  ...,  1.0303e-01,\n",
      "          -1.0651e-01, -1.9934e-01],\n",
      "         [ 1.8219e-02, -2.4072e-01, -8.5327e-02,  ...,  4.9951e-01,\n",
      "           2.6367e-01, -1.9211e-02],\n",
      "         [-9.3689e-02, -2.4524e-01,  9.6924e-02,  ...,  1.4514e-01,\n",
      "           1.8176e-01,  2.9126e-01],\n",
      "         ...,\n",
      "         [-3.3474e-04,  1.6756e-03,  4.2175e-02,  ...,  3.7872e-02,\n",
      "           1.2283e-02,  5.5817e-02],\n",
      "         [ 2.2717e-01,  6.4331e-02,  9.9854e-02,  ...,  4.4281e-02,\n",
      "          -9.8145e-02,  2.6782e-01],\n",
      "         [ 1.0468e-01,  9.9182e-02,  3.0014e-02,  ...,  1.8347e-01,\n",
      "          -9.0698e-02,  2.9492e-01]]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import re\n",
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the tokenizer & model\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/ProstT5', do_lower_case=False) #.to(device)\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/ProstT5\").to(device)\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "model.full() if device=='cpu' else model.half()\n",
    "\n",
    "# prepare your protein sequences/structures as a list. Amino acid sequences are expected to be upper-case (\"PRTEINO\" below) while 3Di-sequences need to be lower-case (\"strctr\" below).\n",
    "sequence_examples = [\"PRTEINO\", \"PRTEINO\", \"strct\"]\n",
    "\n",
    "# replace all rare/ambiguous amino acids by X (3Di sequences does not have those) and introduce white-space between all sequences (AAs and 3Di)\n",
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "# add pre-fixes accordingly (this already expects 3Di-sequences to be lower-case)\n",
    "# if you go from AAs to 3Di (or if you want to embed AAs), you need to prepend \"<AA2fold>\"\n",
    "# if you go from 3Di to AAs (or if you want to embed 3Di), you need to prepend \"<fold2AA>\"\n",
    "sequence_examples = [ \"<AA2fold>\" + \" \" + s if s.isupper() else \"<fold2AA>\" + \" \" + s\n",
    "                      for s in sequence_examples\n",
    "                    ]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids = tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, padding=\"longest\",return_tensors='pt').to(device)\n",
    "\n",
    "# generate embeddings\n",
    "with torch.no_grad():\n",
    "    embedding_repr = model(\n",
    "              ids.input_ids, \n",
    "              attention_mask=ids.attention_mask\n",
    "              )\n",
    "\n",
    "# extract residue embeddings for the first ([0,:]) sequence in the batch and remove padded & special tokens, incl. prefix ([0,1:8]) \n",
    "emb_0 = embedding_repr.last_hidden_state[0,1:8] # shape (7 x 1024)\n",
    "print(embedding_repr.last_hidden_state.shape)\n",
    "print(embedding_repr.last_hidden_state)\n",
    "\n",
    "# same for the second ([1,:]) sequence but taking into account different sequence lengths ([1,:6])\n",
    "emb_1 = embedding_repr.last_hidden_state[1,1:6] # shape (5 x 1024)\n",
    "\n",
    "# if you want to derive a single representation (per-protein embedding) for the whole protein\n",
    "emb_0_per_protein = emb_0.mean(dim=0) # shape (1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9, 1024])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_repr.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1263, -0.0712,  0.0474,  ...,  0.2103, -0.0312, -0.0347],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "print(emb_0_per_protein)\n",
    "print(emb_0_per_protein.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1025, -0.0829,  0.0972,  ...,  0.0512,  0.1245,  0.1018],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "emb_1_per_protein = emb_1.mean(dim=0) # shape (1024)\n",
    "print(emb_1_per_protein)\n",
    "print(emb_1_per_protein.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_repr.last_hidden_state[0,1:].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumgnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
