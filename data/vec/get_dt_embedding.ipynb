{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../nodefeaturing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drug feature extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "original_path = sys.path.copy()\n",
    "sys.path.insert(0, '../../nodefeaturing')\n",
    "import generate_entity_embedding\n",
    "importlib.reload(generate_entity_embedding)\n",
    "sys.path = original_path\n",
    "from generate_entity_embedding import generating_pro_feature, generating_drug_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' drugbankid2smiles '''\n",
    "file_path = \"dti2vec_drugbankid2smiles\"\n",
    "df = pd.read_csv(file_path, header=None, delimiter=\"\\t\")  # Assuming the file has no header row\n",
    "print(len(df))\n",
    "print(df.iloc[[0,1,300,600,1000]])\n",
    "df.columns = [\"Drug_ID\", \"Drug\"]\n",
    "\n",
    "''' Load json file for mapping encoding & Drug ID '''\n",
    "temp = []\n",
    "json_file = 'node2id.json'\n",
    "with open(json_file) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for i_idx, item in enumerate(df[\"Drug_ID\"].copy()):\n",
    "    for j_idx, d_name in enumerate(data.keys()):\n",
    "        if d_name == item:\n",
    "            temp.append(data[d_name])\n",
    "            break\n",
    "        elif j_idx +1 == len(data):\n",
    "            temp.append(np.NaN)\n",
    "\n",
    "''' get drug features '''\n",
    "# Select unique rows based on the 'Drug' column\n",
    "unique_drugs_df = df.drop_duplicates(subset=['Drug']).dropna()\n",
    "unique_drugs_df = generating_drug_feature(unique_drugs_df)\n",
    "# print(len(unique_drugs_df))\n",
    "# print(unique_drugs_df)\n",
    "tmp = unique_drugs_df[['Drug_ID', 'Drug','MORGAN_Features']]\n",
    "drug_id_array = tmp['Drug_ID'].values\n",
    "drug_array = tmp['Drug'].values\n",
    "drug_feat = tmp['MORGAN_Features']\n",
    "drug_enco = temp\n",
    "\n",
    "drug_feat = {'Drug_ID': drug_id_array, 'Drug': drug_array, 'MORGAN_Features': drug_feat, \"Drug_enco\" : drug_enco}\n",
    "with open(\"VEC_drug_feats_MORGAN.pkl\", 'wb') as f:\n",
    "   pickle.dump(drug_feat, f)\n",
    "print(f\"Dictionary saved as pickle file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Drug_ID', 'Drug', 'Morgan_Features', 'Drug_enco'])\n",
      "(1478,)\n",
      "['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'list'>\n",
      "dict_keys(['Drug_ID', 'Drug', 'MORGAN_Features', 'Drug_enco'])\n",
      "(1478,)\n",
      "['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('VEC_drug_feats.pkl', 'rb') as f:\n",
    "    old = pickle.load(f, encoding='utf-8')\n",
    "print(old.keys())\n",
    "print(old['Morgan_Features'].shape)\n",
    "print(old['Morgan_Features'][0])\n",
    "print(type(old['Morgan_Features']))\n",
    "print(type(old['Morgan_Features'][0]))\n",
    "\n",
    "with open('VEC_drug_feats_MORGAN.pkl', 'rb') as f:\n",
    "    new = pickle.load(f, encoding='utf-8')\n",
    "print(new.keys())\n",
    "print(new['MORGAN_Features'].shape)\n",
    "print(new['MORGAN_Features'][0])\n",
    "print(type(new['MORGAN_Features']))\n",
    "print(type(new['MORGAN_Features'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Drug_ID', 'Drug', 'Morgan_Features', 'Drug_enco'])\n",
      "{'Drug_ID': array(['DB00672', 'DB06663', 'DB00586', ..., 'DB00610', 'DB08903',\n",
      "       'DB00474'], dtype=object), 'Drug': array(['CCCNC(=O)NS(=O)(=O)C1=CC=C(C=C1)Cl',\n",
      "       'C1[C@H](CN2[C@@H]1C(=O)N[C@H](C(=O)N[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C2=O)CC3=CC=CC=C3)CC4=CC=C(C=C4)OCC5=CC=CC=C5)CCCCN)CC6=CNC7=CC=CC=C76)C8=CC=CC=C8)OC(=O)NCCN',\n",
      "       'C1=CC=C(C(=C1)CC(=O)O)NC2=C(C=CC=C2Cl)Cl', ...,\n",
      "       'C[C@@H]([C@@H](C1=CC(=CC=C1)O)O)N',\n",
      "       'CN(C)CC[C@@](C1=CC=CC2=CC=CC=C21)([C@H](C3=CC=CC=C3)C4=C(N=C5C=CC(=CC5=C4)Br)OC)O',\n",
      "       'CCC#CC(C)C1(C(=O)NC(=O)N(C1=O)C)CC=C'], dtype=object), 'Morgan_Features': array([array([0.28844152, 0.03085759, 0.33230764, ..., 0.06655665, 0.05585421,\n",
      "       0.00247426]),\n",
      "       array([0.05462653, 0.3620248 , 0.11030577, ..., 0.31672817, 0.16140309,\n",
      "       0.17364596]),\n",
      "       array([0.13667219, 0.01604207, 0.00904966, ..., 0.16250673, 0.2598468 ,\n",
      "       0.20769013]),\n",
      "       ...,\n",
      "       array([0.22865871, 0.0650083 , 0.06557927, ..., 0.06443158, 0.25127315,\n",
      "       0.17943124]),\n",
      "       array([0.02194529, 0.03820903, 0.01996569, ..., 0.36102689, 0.12834221,\n",
      "       0.29452861]),\n",
      "       array([0.05973068, 0.00625003, 0.13203485, ..., 0.03436274, 0.20000474,\n",
      "       0.15009501])], dtype=object), 'Drug_enco': array([ 569., 1030.,  668., ...,  728., 1315.,  240.])}\n",
      "dict_keys(['Drug_ID', 'Drug', 'Drug_enco', 'MAP4_Features'])\n",
      "{'Drug_ID': array(['DB00672', 'DB06663', 'DB00586', ..., 'DB00610', 'DB08903',\n",
      "       'DB00474'], dtype=object), 'Drug': array(['CCCNC(=O)NS(=O)(=O)C1=CC=C(C=C1)Cl',\n",
      "       'C1[C@H](CN2[C@@H]1C(=O)N[C@H](C(=O)N[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C2=O)CC3=CC=CC=C3)CC4=CC=C(C=C4)OCC5=CC=CC=C5)CCCCN)CC6=CNC7=CC=CC=C76)C8=CC=CC=C8)OC(=O)NCCN',\n",
      "       'C1=CC=C(C(=C1)CC(=O)O)NC2=C(C=CC=C2Cl)Cl', ...,\n",
      "       'C[C@@H]([C@@H](C1=CC(=CC=C1)O)O)N',\n",
      "       'CN(C)CC[C@@](C1=CC=CC2=CC=CC=C21)([C@H](C3=CC=CC=C3)C4=C(N=C5C=CC(=CC5=C4)Br)OC)O',\n",
      "       'CCC#CC(C)C1(C(=O)NC(=O)N(C1=O)C)CC=C'], dtype=object), 'Drug_enco': array([ 569., 1030.,  668., ...,  728., 1315.,  240.]), 'MAP4_Features': array([array([0.28844152, 0.03085759, 0.33230764, ..., 0.06655665, 0.05585421,\n",
      "       0.00247426]),\n",
      "       array([0.05462653, 0.3620248 , 0.11030577, ..., 0.31672817, 0.16140309,\n",
      "       0.17364596]),\n",
      "       array([0.13667219, 0.01604207, 0.00904966, ..., 0.16250673, 0.2598468 ,\n",
      "       0.20769013]),\n",
      "       ...,\n",
      "       array([0.22865871, 0.0650083 , 0.06557927, ..., 0.06443158, 0.25127315,\n",
      "       0.17943124]),\n",
      "       array([0.02194529, 0.03820903, 0.01996569, ..., 0.36102689, 0.12834221,\n",
      "       0.29452861]),\n",
      "       array([0.05973068, 0.00625003, 0.13203485, ..., 0.03436274, 0.20000474,\n",
      "       0.15009501])], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('VEC_drug_feats_map4.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "print(x.keys())\n",
    "print(x)\n",
    "x['MAP4_Features'] = x.pop('Morgan_Features')\n",
    "print(x.keys())\n",
    "print(x)\n",
    "with open(\"VEC_drug_feats_map4.pkl\", 'wb') as f:\n",
    "   pickle.dump(x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Drug_ID', 'Drug', 'Drug_enco', 'MAP4_Features'])\n",
      "{'Drug_ID': array(['DB00672', 'DB06663', 'DB00586', ..., 'DB00610', 'DB08903',\n",
      "       'DB00474'], dtype=object), 'Drug': array(['CCCNC(=O)NS(=O)(=O)C1=CC=C(C=C1)Cl',\n",
      "       'C1[C@H](CN2[C@@H]1C(=O)N[C@H](C(=O)N[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C2=O)CC3=CC=CC=C3)CC4=CC=C(C=C4)OCC5=CC=CC=C5)CCCCN)CC6=CNC7=CC=CC=C76)C8=CC=CC=C8)OC(=O)NCCN',\n",
      "       'C1=CC=C(C(=C1)CC(=O)O)NC2=C(C=CC=C2Cl)Cl', ...,\n",
      "       'C[C@@H]([C@@H](C1=CC(=CC=C1)O)O)N',\n",
      "       'CN(C)CC[C@@](C1=CC=CC2=CC=CC=C21)([C@H](C3=CC=CC=C3)C4=C(N=C5C=CC(=CC5=C4)Br)OC)O',\n",
      "       'CCC#CC(C)C1(C(=O)NC(=O)N(C1=O)C)CC=C'], dtype=object), 'Drug_enco': array([ 569., 1030.,  668., ...,  728., 1315.,  240.]), 'MAP4_Features': array([array([0.28844152, 0.03085759, 0.33230764, ..., 0.06655665, 0.05585421,\n",
      "       0.00247426]),\n",
      "       array([0.05462653, 0.3620248 , 0.11030577, ..., 0.31672817, 0.16140309,\n",
      "       0.17364596]),\n",
      "       array([0.13667219, 0.01604207, 0.00904966, ..., 0.16250673, 0.2598468 ,\n",
      "       0.20769013]),\n",
      "       ...,\n",
      "       array([0.22865871, 0.0650083 , 0.06557927, ..., 0.06443158, 0.25127315,\n",
      "       0.17943124]),\n",
      "       array([0.02194529, 0.03820903, 0.01996569, ..., 0.36102689, 0.12834221,\n",
      "       0.29452861]),\n",
      "       array([0.05973068, 0.00625003, 0.13203485, ..., 0.03436274, 0.20000474,\n",
      "       0.15009501])], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "with open('VEC_drug_feats_map4.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "print(x.keys())\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Drug_ID', 'Drug', 'Morgan_Features', 'Drug_enco'])\n",
      "{'Drug_ID': array(['DB00672', 'DB00116', 'DB06663', ..., 'DB00610', 'DB08903',\n",
      "       'DB00474'], dtype=object), 'Drug': array(['CCCNC(=O)NS(=O)(=O)C1=CC=C(C=C1)Cl',\n",
      "       'C1C(NC2=C(N1)N=C(NC2=O)N)CNC3=CC=C(C=C3)C(=O)N[C@@H](CCC(=O)O)C(=O)O',\n",
      "       'C1[C@H](CN2[C@@H]1C(=O)N[C@H](C(=O)N[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C2=O)CC3=CC=CC=C3)CC4=CC=C(C=C4)OCC5=CC=CC=C5)CCCCN)CC6=CNC7=CC=CC=C76)C8=CC=CC=C8)OC(=O)NCCN',\n",
      "       ..., 'C[C@@H]([C@@H](C1=CC(=CC=C1)O)O)N',\n",
      "       'CN(C)CC[C@@](C1=CC=CC2=CC=CC=C21)([C@H](C3=CC=CC=C3)C4=C(N=C5C=CC(=CC5=C4)Br)OC)O',\n",
      "       'CCC#CC(C)C1(C(=O)NC(=O)N(C1=O)C)CC=C'], dtype=object), 'Morgan_Features': 0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...\n",
      "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "6       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "                              ...                        \n",
      "9640    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9751    [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9760    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9787    [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9801    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: Morgan_Features, Length: 1478, dtype: object, 'Drug_enco': [569, nan, 1030, 668, 804, 231, 600, 228, 373, 164, 985, nan, 407, 1480, 234, 175, 166, 1080, nan, 580, 374, 677, 987, 712, 1633, 501, 280, 1049, 281, 482, 272, 1316, 509, 1634, 537, 967, 1078, nan, 270, 558, 443, 347, 779, 165, 468, 680, 823, 258, 1261, 450, 816, 1202, 1112, 460, 641, 532, 955, 586, 587, 1427, 813, 1154, 581, nan, 1568, nan, 198, 251, nan, 621, 645, 1019, 1578, 590, 608, 629, 31, 477, 992, 319, 326, 103, 1275, 332, 181, 143, 567, 976, 221, 294, 669, nan, 695, 530, 135, 156, 196, 709, 1260, 1201, 1196, 189, 202, 208, 664, 1118, 438, 414, 929, nan, 869, 937, 320, 266, 496, 467, 444, 458, 483, 857, 1020, 47, 515, 302, 200, 1306, 1058, 977, 536, 394, 1620, 740, 952, nan, 255, 884, 473, 619, 1299, 512, 1709, 516, nan, 1088, 193, 125, 684, 861, 1421, nan, 556, 161, 371, 235, 932, 598, 1304, 60, 750, 437, 1084, 1168, 59, 634, 1361, 1295, 681, 32, 704, 914, 80, 188, 620, 474, 1504, 448, 961, 354, 307, 593, 486, 44, 1006, 262, 648, 653, 76, 142, 343, 179, 585, 666, 1176, 472, 1254, 170, 1212, 316, 369, 404, 622, 194, 157, 1270, 92, 225, 748, 1156, 1481, 377, 676, 521, nan, 1138, nan, 308, 1437, 912, 1310, 1220, 523, 370, 882, 488, 315, 229, 53, 544, nan, 119, 260, 1251, 887, 158, 628, 1117, 1069, 871, 894, 1533, 724, 527, 1033, 1277, 1369, 659, 616, 1426, 820, 259, 39, 1278, 1428, 714, 1524, 1218, 18, 731, 886, 675, 63, 1133, 230, 275, 12, 1086, 691, 312, 264, 726, 982, 968, 405, nan, 1012, 916, 1091, 1208, 1256, 1348, 226, 565, 350, 577, 981, 267, 881, 415, 1131, 248, 959, 317, 499, 1157, 359, 487, 1585, 678, 273, 360, 1199, 299, 1219, 334, 392, 1169, 431, 661, 253, 757, 358, 203, 944, 357, 292, 503, 352, 1566, 570, 257, 201, nan, 533, 298, 926, nan, 764, 1537, 297, 1435, 540, 1335, 1098, 147, nan, 1170, 550, 440, 282, 172, 283, 1134, 26, nan, 171, 1081, 979, 207, nan, 574, 1470, 336, 839, 290, nan, 504, 836, 1455, 989, 805, 1367, 1197, 497, 526, 606, 364, 413, 162, 288, 1697, 428, 244, 233, 127, 22, 883, 694, 471, 177, 543, 983, nan, 35, 674, 834, 1036, 435, 199, 736, 1475, 755, 738, 1407, 618, 555, 721, 388, 739, 287, 673, 588, 252, 642, 1120, 1416, 1590, 1007, 276, 1328, 180, 159, 51, 79, 1401, nan, 1293, 1627, 106, nan, 1419, 416, 1115, 1243, 1695, 925, 713, 610, nan, nan, 785, 851, 1111, 1040, 1126, 1358, 67, 1282, 928, 1226, nan, 1085, 1207, 660, 69, 1074, 637, 457, 576, 500, 219, 939, 396, 70, 408, 566, 1354, 329, 1392, 153, 190, 247, 239, 859, 139, 98, 1271, 559, nan, 232, 330, 417, 1366, 454, 261, 1172, 730, 1123, 1001, 1094, 644, 120, nan, nan, 1323, 1430, 639, 1317, 1107, 439, 706, 592, 1011, 1648, 20, 187, 446, 495, 385, 274, 650, nan, 1287, 1456, 812, 1070, 1318, 335, 998, 209, 452, 786, 277, 400, 954, 1023, 1536, 409, 1184, 999, 700, nan, 1395, 346, 46, nan, 384, 124, 245, 831, 1389, 1341, 491, 885, nan, nan, 1060, 131, 1124, 1305, 455, 16, 601, 1145, 1359, 1093, 426, 1379, 447, 353, 640, 38, 866, 55, 901, 28, 484, 878, 1388, 379, 1614, 1400, 362, 72, 617, 571, 1408, 1026, 510, 442, nan, 611, 931, 563, 1301, 397, 1185, 41, 822, 578, 42, 195, 670, 478, 197, nan, 1279, 1671, 163, 663, 1281, 1343, 1373, 321, 430, 144, 1099, 1213, 137, 1032, 1018, 1376, nan, 1171, 83, 1386, 1229, 1378, 824, nan, 864, 529, 1233, 1531, 1532, 1037, 268, 561, 524, 507, 237, 531, 453, 1186, 705, 309, 1549, 671, 269, 940, 828, 465, 97, nan, 1474, 1095, nan, 214, 562, 632, 658, 656, 1189, 412, nan, 795, 327, 1615, nan, nan, 840, 652, 1268, 514, 459, 1097, nan, 1200, 224, 182, 278, nan, 1493, 907, 548, nan, 1402, 1068, 1272, 84, 434, 729, 818, 1090, 597, 57, 635, 568, 1136, nan, 322, 1410, nan, 1551, 82, 972, nan, nan, 1629, 609, 633, 490, 845, 698, 842, 522, 141, 542, 718, 1355, 686, 614, 324, 811, 1332, 1263, 420, 799, 1127, 1003, 1010, 575, 1646, 376, 43, 250, nan, 236, nan, 341, 1104, 206, 289, 1441, 451, 391, 1288, 505, 4, 549, nan, 525, 1028, 111, 295, nan, 1225, 424, 1653, 243, 1526, 1284, 1424, 1072, 238, 1031, nan, 1223, 210, 303, 1017, 655, nan, 715, 1631, 64, 1191, 1244, 1231, 1130, 30, 494, 121, 703, 429, 1022, 1173, 466, 1346, 492, 1494, 599, 1422, nan, 29, 21, 951, 1240, 604, 14, 508, nan, 973, 314, 909, 1298, 1655, 897, 375, 1057, 1079, 1433, 1055, nan, 1162, 1296, 541, 65, 651, 1014, 1308, 589, 410, 613, 892, 1210, 383, 432, 631, 602, 77, 1636, 774, 767, 835, 711, 689, 1309, nan, 749, 263, 546, 1110, nan, 934, nan, 36, 122, 806, 1067, 7, 1248, 1357, 481, 743, 1350, 1639, 964, 304, nan, 1224, 787, 1406, 390, 688, 1062, 560, 808, 1077, nan, 1675, 539, nan, nan, nan, 1265, 105, nan, 401, 56, 970, 310, 564, 1345, 647, 1663, 387, 1038, 441, 1352, 654, 68, 109, 1338, 1466, 183, 1016, 318, 1238, 34, 1507, 93, 1241, nan, 403, 138, 74, 518, nan, 1203, 880, 1477, 1076, 535, 991, 803, 249, 1071, 636, 1616, 1431, 1439, 978, 554, 506, 657, 223, 27, 1242, 296, 1021, 445, nan, nan, 1651, 284, 920, nan, 150, 1051, 345, 801, nan, 528, 513, 1362, 152, 623, 1542, nan, nan, 693, 285, 205, 915, 1015, 708, 572, 340, 1215, 867, nan, 485, nan, 662, 1425, 23, 114, 186, 980, 227, 1465, nan, 216, 265, 62, 1228, 1046, 386, 1365, 242, 717, 365, 933, 603, 1054, 1705, 311, 679, 1181, 50, 974, 596, 643, nan, 752, 699, 254, 630, 463, 363, 1574, 1383, 1486, nan, 1418, 889, 1283, 1517, 1190, 464, 502, 1525, 1065, 10, 81, 1562, 395, 1681, nan, 832, 852, 765, 1322, 54, 212, 1048, 1403, 798, 116, 1529, 1291, nan, 1472, 1511, 1042, 1230, 1029, nan, 423, 215, 99, 1039, 1409, nan, 192, 702, 1108, 1396, 1050, 461, nan, 1314, 1002, 638, 349, 775, 204, 1556, 2, nan, 313, 1412, nan, 672, 900, 1640, 582, 155, 128, 1385, 389, 1342, 140, 1125, 1660, 48, 1445, 40, 433, 1227, 1082, nan, 167, 1307, 517, 1482, 735, 1500, 1320, 971, 1478, 741, 1109, 519, 361, 151, 19, 1372, 425, 301, 351, 1259, 1143, 584, 191, 594, 753, 947, 579, 607, 1669, 919, 1334, 626, 1668, 291, 1027, 271, 328, 462, 176, 279, 1092, 893, 58, 727, 942, 846, 1035, nan, 96, 615, 300, 1484, 860, 1247, 11, nan, 1100, 720, 1178, 1234, 1161, 1187, 1547, 149, 1545, 1670, 145, 45, 1382, 355, 342, 1237, 1179, 965, 891, 211, 1641, 1654, 498, 745, 1121, 732, 1624, 797, nan, 1174, nan, 168, 101, 1688, 1488, 1083, 1321, nan, 911, 858, nan, 551, nan, 1129, nan, nan, 1327, nan, 762, 719, nan, 1297, 924, 1630, 1696, nan, nan, 1693, 1303, 830, 768, 1116, 1089, 338, 286, 534, 511, 1508, 1034, 722, 1004, 398, 850, 1368, 1515, 1045, 469, 1325, 761, 1609, 1520, 697, 844, 378, 1236, 331, 975, 411, 692, 957, 1180, 744, nan, 1375, 132, 344, 372, 1381, 862, 1292, 756, nan, 591, nan, 1114, 1005, 1499, 427, 1643, 1581, 723, 833, 1447, 476, 1135, 154, nan, nan, 1656, 173, 222, nan, 1394, 475, 1349, nan, 769, 73, 1680, 956, 1158, 1637, 17, 941, 218, 829, 493, 169, 1674, 1509, nan, nan, 117, 953, 849, nan, nan, nan, 115, 1538, 984, 1450, 1066, 784, 1257, 1464, 547, 305, 1175, 1324, 419, 1267, 649, nan, 1679, 754, nan, 545, 716, 1047, nan, nan, 1192, 479, 1420, nan, 113, 130, nan, nan, 1519, 994, 348, 104, 1141, 1290, nan, nan, 1177, nan, 123, 1132, 1044, 1502, 1326, 1052, 990, 1294, 1221, 1489, 1339, 333, nan, 792, 87, 380, nan, 1626, 1353, 1337, 1159, 710, 1621, 938, 1678, 1235, 966, 184, 1458, 1440, 1446, 75, 1252, nan, 107, 1591, 1521, 917, 782, 1087, 1209, 780, nan, 37, 1059, nan, 969, 174, 1061, 1404, nan, 1596, 1415, nan, 1103, nan, 1360, 1311, 1451, 1576, 24, nan, nan, 821, 421, 962, 1289, 489, 1113, 1604, 997, 1479, 1510, 1149, 945, 1552, 1473, 1652, 1105, 1587, 1053, 1102, 306, 874, 1364, nan, 1423, 809, 1483, 1000, 1214, 1398, 1347, 1393, 1468, 25, 133, 665, nan, 393, 1448, 682, 339, 557, 758, 1285, nan, 552, 1043, 1344, nan, 1024, 1650, 1232, nan, 1649, 110, 1041, 1672, 1013, 1457, 879, 1064, 1544, nan, 865, 725, 1535, 921, 1497, 1708, 751, 456, 1008, 800, 1258, 949, 583, nan, 890, 1550, 1211, 1165, 1255, 1262, 15, 1667, 1319, nan, 1595, 802, 788, 1063, 827, 1148, 1498, 734, 134, 1518, 1610, nan, 1249, 1530, nan, nan, 895, 1188, 728, 1315, 240]}\n",
      "dict_keys(['Drug_ID', 'Drug', 'Drug_enco', 'MORGAN_Features'])\n",
      "{'Drug_ID': array(['DB00672', 'DB00116', 'DB06663', ..., 'DB00610', 'DB08903',\n",
      "       'DB00474'], dtype=object), 'Drug': array(['CCCNC(=O)NS(=O)(=O)C1=CC=C(C=C1)Cl',\n",
      "       'C1C(NC2=C(N1)N=C(NC2=O)N)CNC3=CC=C(C=C3)C(=O)N[C@@H](CCC(=O)O)C(=O)O',\n",
      "       'C1[C@H](CN2[C@@H]1C(=O)N[C@H](C(=O)N[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C2=O)CC3=CC=CC=C3)CC4=CC=C(C=C4)OCC5=CC=CC=C5)CCCCN)CC6=CNC7=CC=CC=C76)C8=CC=CC=C8)OC(=O)NCCN',\n",
      "       ..., 'C[C@@H]([C@@H](C1=CC(=CC=C1)O)O)N',\n",
      "       'CN(C)CC[C@@](C1=CC=CC2=CC=CC=C21)([C@H](C3=CC=CC=C3)C4=C(N=C5C=CC(=CC5=C4)Br)OC)O',\n",
      "       'CCC#CC(C)C1(C(=O)NC(=O)N(C1=O)C)CC=C'], dtype=object), 'Drug_enco': [569, nan, 1030, 668, 804, 231, 600, 228, 373, 164, 985, nan, 407, 1480, 234, 175, 166, 1080, nan, 580, 374, 677, 987, 712, 1633, 501, 280, 1049, 281, 482, 272, 1316, 509, 1634, 537, 967, 1078, nan, 270, 558, 443, 347, 779, 165, 468, 680, 823, 258, 1261, 450, 816, 1202, 1112, 460, 641, 532, 955, 586, 587, 1427, 813, 1154, 581, nan, 1568, nan, 198, 251, nan, 621, 645, 1019, 1578, 590, 608, 629, 31, 477, 992, 319, 326, 103, 1275, 332, 181, 143, 567, 976, 221, 294, 669, nan, 695, 530, 135, 156, 196, 709, 1260, 1201, 1196, 189, 202, 208, 664, 1118, 438, 414, 929, nan, 869, 937, 320, 266, 496, 467, 444, 458, 483, 857, 1020, 47, 515, 302, 200, 1306, 1058, 977, 536, 394, 1620, 740, 952, nan, 255, 884, 473, 619, 1299, 512, 1709, 516, nan, 1088, 193, 125, 684, 861, 1421, nan, 556, 161, 371, 235, 932, 598, 1304, 60, 750, 437, 1084, 1168, 59, 634, 1361, 1295, 681, 32, 704, 914, 80, 188, 620, 474, 1504, 448, 961, 354, 307, 593, 486, 44, 1006, 262, 648, 653, 76, 142, 343, 179, 585, 666, 1176, 472, 1254, 170, 1212, 316, 369, 404, 622, 194, 157, 1270, 92, 225, 748, 1156, 1481, 377, 676, 521, nan, 1138, nan, 308, 1437, 912, 1310, 1220, 523, 370, 882, 488, 315, 229, 53, 544, nan, 119, 260, 1251, 887, 158, 628, 1117, 1069, 871, 894, 1533, 724, 527, 1033, 1277, 1369, 659, 616, 1426, 820, 259, 39, 1278, 1428, 714, 1524, 1218, 18, 731, 886, 675, 63, 1133, 230, 275, 12, 1086, 691, 312, 264, 726, 982, 968, 405, nan, 1012, 916, 1091, 1208, 1256, 1348, 226, 565, 350, 577, 981, 267, 881, 415, 1131, 248, 959, 317, 499, 1157, 359, 487, 1585, 678, 273, 360, 1199, 299, 1219, 334, 392, 1169, 431, 661, 253, 757, 358, 203, 944, 357, 292, 503, 352, 1566, 570, 257, 201, nan, 533, 298, 926, nan, 764, 1537, 297, 1435, 540, 1335, 1098, 147, nan, 1170, 550, 440, 282, 172, 283, 1134, 26, nan, 171, 1081, 979, 207, nan, 574, 1470, 336, 839, 290, nan, 504, 836, 1455, 989, 805, 1367, 1197, 497, 526, 606, 364, 413, 162, 288, 1697, 428, 244, 233, 127, 22, 883, 694, 471, 177, 543, 983, nan, 35, 674, 834, 1036, 435, 199, 736, 1475, 755, 738, 1407, 618, 555, 721, 388, 739, 287, 673, 588, 252, 642, 1120, 1416, 1590, 1007, 276, 1328, 180, 159, 51, 79, 1401, nan, 1293, 1627, 106, nan, 1419, 416, 1115, 1243, 1695, 925, 713, 610, nan, nan, 785, 851, 1111, 1040, 1126, 1358, 67, 1282, 928, 1226, nan, 1085, 1207, 660, 69, 1074, 637, 457, 576, 500, 219, 939, 396, 70, 408, 566, 1354, 329, 1392, 153, 190, 247, 239, 859, 139, 98, 1271, 559, nan, 232, 330, 417, 1366, 454, 261, 1172, 730, 1123, 1001, 1094, 644, 120, nan, nan, 1323, 1430, 639, 1317, 1107, 439, 706, 592, 1011, 1648, 20, 187, 446, 495, 385, 274, 650, nan, 1287, 1456, 812, 1070, 1318, 335, 998, 209, 452, 786, 277, 400, 954, 1023, 1536, 409, 1184, 999, 700, nan, 1395, 346, 46, nan, 384, 124, 245, 831, 1389, 1341, 491, 885, nan, nan, 1060, 131, 1124, 1305, 455, 16, 601, 1145, 1359, 1093, 426, 1379, 447, 353, 640, 38, 866, 55, 901, 28, 484, 878, 1388, 379, 1614, 1400, 362, 72, 617, 571, 1408, 1026, 510, 442, nan, 611, 931, 563, 1301, 397, 1185, 41, 822, 578, 42, 195, 670, 478, 197, nan, 1279, 1671, 163, 663, 1281, 1343, 1373, 321, 430, 144, 1099, 1213, 137, 1032, 1018, 1376, nan, 1171, 83, 1386, 1229, 1378, 824, nan, 864, 529, 1233, 1531, 1532, 1037, 268, 561, 524, 507, 237, 531, 453, 1186, 705, 309, 1549, 671, 269, 940, 828, 465, 97, nan, 1474, 1095, nan, 214, 562, 632, 658, 656, 1189, 412, nan, 795, 327, 1615, nan, nan, 840, 652, 1268, 514, 459, 1097, nan, 1200, 224, 182, 278, nan, 1493, 907, 548, nan, 1402, 1068, 1272, 84, 434, 729, 818, 1090, 597, 57, 635, 568, 1136, nan, 322, 1410, nan, 1551, 82, 972, nan, nan, 1629, 609, 633, 490, 845, 698, 842, 522, 141, 542, 718, 1355, 686, 614, 324, 811, 1332, 1263, 420, 799, 1127, 1003, 1010, 575, 1646, 376, 43, 250, nan, 236, nan, 341, 1104, 206, 289, 1441, 451, 391, 1288, 505, 4, 549, nan, 525, 1028, 111, 295, nan, 1225, 424, 1653, 243, 1526, 1284, 1424, 1072, 238, 1031, nan, 1223, 210, 303, 1017, 655, nan, 715, 1631, 64, 1191, 1244, 1231, 1130, 30, 494, 121, 703, 429, 1022, 1173, 466, 1346, 492, 1494, 599, 1422, nan, 29, 21, 951, 1240, 604, 14, 508, nan, 973, 314, 909, 1298, 1655, 897, 375, 1057, 1079, 1433, 1055, nan, 1162, 1296, 541, 65, 651, 1014, 1308, 589, 410, 613, 892, 1210, 383, 432, 631, 602, 77, 1636, 774, 767, 835, 711, 689, 1309, nan, 749, 263, 546, 1110, nan, 934, nan, 36, 122, 806, 1067, 7, 1248, 1357, 481, 743, 1350, 1639, 964, 304, nan, 1224, 787, 1406, 390, 688, 1062, 560, 808, 1077, nan, 1675, 539, nan, nan, nan, 1265, 105, nan, 401, 56, 970, 310, 564, 1345, 647, 1663, 387, 1038, 441, 1352, 654, 68, 109, 1338, 1466, 183, 1016, 318, 1238, 34, 1507, 93, 1241, nan, 403, 138, 74, 518, nan, 1203, 880, 1477, 1076, 535, 991, 803, 249, 1071, 636, 1616, 1431, 1439, 978, 554, 506, 657, 223, 27, 1242, 296, 1021, 445, nan, nan, 1651, 284, 920, nan, 150, 1051, 345, 801, nan, 528, 513, 1362, 152, 623, 1542, nan, nan, 693, 285, 205, 915, 1015, 708, 572, 340, 1215, 867, nan, 485, nan, 662, 1425, 23, 114, 186, 980, 227, 1465, nan, 216, 265, 62, 1228, 1046, 386, 1365, 242, 717, 365, 933, 603, 1054, 1705, 311, 679, 1181, 50, 974, 596, 643, nan, 752, 699, 254, 630, 463, 363, 1574, 1383, 1486, nan, 1418, 889, 1283, 1517, 1190, 464, 502, 1525, 1065, 10, 81, 1562, 395, 1681, nan, 832, 852, 765, 1322, 54, 212, 1048, 1403, 798, 116, 1529, 1291, nan, 1472, 1511, 1042, 1230, 1029, nan, 423, 215, 99, 1039, 1409, nan, 192, 702, 1108, 1396, 1050, 461, nan, 1314, 1002, 638, 349, 775, 204, 1556, 2, nan, 313, 1412, nan, 672, 900, 1640, 582, 155, 128, 1385, 389, 1342, 140, 1125, 1660, 48, 1445, 40, 433, 1227, 1082, nan, 167, 1307, 517, 1482, 735, 1500, 1320, 971, 1478, 741, 1109, 519, 361, 151, 19, 1372, 425, 301, 351, 1259, 1143, 584, 191, 594, 753, 947, 579, 607, 1669, 919, 1334, 626, 1668, 291, 1027, 271, 328, 462, 176, 279, 1092, 893, 58, 727, 942, 846, 1035, nan, 96, 615, 300, 1484, 860, 1247, 11, nan, 1100, 720, 1178, 1234, 1161, 1187, 1547, 149, 1545, 1670, 145, 45, 1382, 355, 342, 1237, 1179, 965, 891, 211, 1641, 1654, 498, 745, 1121, 732, 1624, 797, nan, 1174, nan, 168, 101, 1688, 1488, 1083, 1321, nan, 911, 858, nan, 551, nan, 1129, nan, nan, 1327, nan, 762, 719, nan, 1297, 924, 1630, 1696, nan, nan, 1693, 1303, 830, 768, 1116, 1089, 338, 286, 534, 511, 1508, 1034, 722, 1004, 398, 850, 1368, 1515, 1045, 469, 1325, 761, 1609, 1520, 697, 844, 378, 1236, 331, 975, 411, 692, 957, 1180, 744, nan, 1375, 132, 344, 372, 1381, 862, 1292, 756, nan, 591, nan, 1114, 1005, 1499, 427, 1643, 1581, 723, 833, 1447, 476, 1135, 154, nan, nan, 1656, 173, 222, nan, 1394, 475, 1349, nan, 769, 73, 1680, 956, 1158, 1637, 17, 941, 218, 829, 493, 169, 1674, 1509, nan, nan, 117, 953, 849, nan, nan, nan, 115, 1538, 984, 1450, 1066, 784, 1257, 1464, 547, 305, 1175, 1324, 419, 1267, 649, nan, 1679, 754, nan, 545, 716, 1047, nan, nan, 1192, 479, 1420, nan, 113, 130, nan, nan, 1519, 994, 348, 104, 1141, 1290, nan, nan, 1177, nan, 123, 1132, 1044, 1502, 1326, 1052, 990, 1294, 1221, 1489, 1339, 333, nan, 792, 87, 380, nan, 1626, 1353, 1337, 1159, 710, 1621, 938, 1678, 1235, 966, 184, 1458, 1440, 1446, 75, 1252, nan, 107, 1591, 1521, 917, 782, 1087, 1209, 780, nan, 37, 1059, nan, 969, 174, 1061, 1404, nan, 1596, 1415, nan, 1103, nan, 1360, 1311, 1451, 1576, 24, nan, nan, 821, 421, 962, 1289, 489, 1113, 1604, 997, 1479, 1510, 1149, 945, 1552, 1473, 1652, 1105, 1587, 1053, 1102, 306, 874, 1364, nan, 1423, 809, 1483, 1000, 1214, 1398, 1347, 1393, 1468, 25, 133, 665, nan, 393, 1448, 682, 339, 557, 758, 1285, nan, 552, 1043, 1344, nan, 1024, 1650, 1232, nan, 1649, 110, 1041, 1672, 1013, 1457, 879, 1064, 1544, nan, 865, 725, 1535, 921, 1497, 1708, 751, 456, 1008, 800, 1258, 949, 583, nan, 890, 1550, 1211, 1165, 1255, 1262, 15, 1667, 1319, nan, 1595, 802, 788, 1063, 827, 1148, 1498, 734, 134, 1518, 1610, nan, 1249, 1530, nan, nan, 895, 1188, 728, 1315, 240], 'MORGAN_Features': 0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...\n",
      "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "6       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "                              ...                        \n",
      "9640    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9751    [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9760    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9787    [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9801    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: Morgan_Features, Length: 1478, dtype: object}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('VEC_drug_feats_morgan.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "print(x.keys())\n",
    "print(x)\n",
    "x['MORGAN_Features'] = x.pop('Morgan_Features')\n",
    "print(x.keys())\n",
    "print(x)\n",
    "with open(\"VEC_drug_feats_morgan.pkl\", 'wb') as f:\n",
    "   pickle.dump(x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Drug_ID', 'Drug', 'Drug_enco', 'MORGAN_Features'])\n",
      "{'Drug_ID': array(['DB00672', 'DB00116', 'DB06663', ..., 'DB00610', 'DB08903',\n",
      "       'DB00474'], dtype=object), 'Drug': array(['CCCNC(=O)NS(=O)(=O)C1=CC=C(C=C1)Cl',\n",
      "       'C1C(NC2=C(N1)N=C(NC2=O)N)CNC3=CC=C(C=C3)C(=O)N[C@@H](CCC(=O)O)C(=O)O',\n",
      "       'C1[C@H](CN2[C@@H]1C(=O)N[C@H](C(=O)N[C@@H](C(=O)N[C@H](C(=O)N[C@H](C(=O)N[C@H](C2=O)CC3=CC=CC=C3)CC4=CC=C(C=C4)OCC5=CC=CC=C5)CCCCN)CC6=CNC7=CC=CC=C76)C8=CC=CC=C8)OC(=O)NCCN',\n",
      "       ..., 'C[C@@H]([C@@H](C1=CC(=CC=C1)O)O)N',\n",
      "       'CN(C)CC[C@@](C1=CC=CC2=CC=CC=C21)([C@H](C3=CC=CC=C3)C4=C(N=C5C=CC(=CC5=C4)Br)OC)O',\n",
      "       'CCC#CC(C)C1(C(=O)NC(=O)N(C1=O)C)CC=C'], dtype=object), 'Drug_enco': [569, nan, 1030, 668, 804, 231, 600, 228, 373, 164, 985, nan, 407, 1480, 234, 175, 166, 1080, nan, 580, 374, 677, 987, 712, 1633, 501, 280, 1049, 281, 482, 272, 1316, 509, 1634, 537, 967, 1078, nan, 270, 558, 443, 347, 779, 165, 468, 680, 823, 258, 1261, 450, 816, 1202, 1112, 460, 641, 532, 955, 586, 587, 1427, 813, 1154, 581, nan, 1568, nan, 198, 251, nan, 621, 645, 1019, 1578, 590, 608, 629, 31, 477, 992, 319, 326, 103, 1275, 332, 181, 143, 567, 976, 221, 294, 669, nan, 695, 530, 135, 156, 196, 709, 1260, 1201, 1196, 189, 202, 208, 664, 1118, 438, 414, 929, nan, 869, 937, 320, 266, 496, 467, 444, 458, 483, 857, 1020, 47, 515, 302, 200, 1306, 1058, 977, 536, 394, 1620, 740, 952, nan, 255, 884, 473, 619, 1299, 512, 1709, 516, nan, 1088, 193, 125, 684, 861, 1421, nan, 556, 161, 371, 235, 932, 598, 1304, 60, 750, 437, 1084, 1168, 59, 634, 1361, 1295, 681, 32, 704, 914, 80, 188, 620, 474, 1504, 448, 961, 354, 307, 593, 486, 44, 1006, 262, 648, 653, 76, 142, 343, 179, 585, 666, 1176, 472, 1254, 170, 1212, 316, 369, 404, 622, 194, 157, 1270, 92, 225, 748, 1156, 1481, 377, 676, 521, nan, 1138, nan, 308, 1437, 912, 1310, 1220, 523, 370, 882, 488, 315, 229, 53, 544, nan, 119, 260, 1251, 887, 158, 628, 1117, 1069, 871, 894, 1533, 724, 527, 1033, 1277, 1369, 659, 616, 1426, 820, 259, 39, 1278, 1428, 714, 1524, 1218, 18, 731, 886, 675, 63, 1133, 230, 275, 12, 1086, 691, 312, 264, 726, 982, 968, 405, nan, 1012, 916, 1091, 1208, 1256, 1348, 226, 565, 350, 577, 981, 267, 881, 415, 1131, 248, 959, 317, 499, 1157, 359, 487, 1585, 678, 273, 360, 1199, 299, 1219, 334, 392, 1169, 431, 661, 253, 757, 358, 203, 944, 357, 292, 503, 352, 1566, 570, 257, 201, nan, 533, 298, 926, nan, 764, 1537, 297, 1435, 540, 1335, 1098, 147, nan, 1170, 550, 440, 282, 172, 283, 1134, 26, nan, 171, 1081, 979, 207, nan, 574, 1470, 336, 839, 290, nan, 504, 836, 1455, 989, 805, 1367, 1197, 497, 526, 606, 364, 413, 162, 288, 1697, 428, 244, 233, 127, 22, 883, 694, 471, 177, 543, 983, nan, 35, 674, 834, 1036, 435, 199, 736, 1475, 755, 738, 1407, 618, 555, 721, 388, 739, 287, 673, 588, 252, 642, 1120, 1416, 1590, 1007, 276, 1328, 180, 159, 51, 79, 1401, nan, 1293, 1627, 106, nan, 1419, 416, 1115, 1243, 1695, 925, 713, 610, nan, nan, 785, 851, 1111, 1040, 1126, 1358, 67, 1282, 928, 1226, nan, 1085, 1207, 660, 69, 1074, 637, 457, 576, 500, 219, 939, 396, 70, 408, 566, 1354, 329, 1392, 153, 190, 247, 239, 859, 139, 98, 1271, 559, nan, 232, 330, 417, 1366, 454, 261, 1172, 730, 1123, 1001, 1094, 644, 120, nan, nan, 1323, 1430, 639, 1317, 1107, 439, 706, 592, 1011, 1648, 20, 187, 446, 495, 385, 274, 650, nan, 1287, 1456, 812, 1070, 1318, 335, 998, 209, 452, 786, 277, 400, 954, 1023, 1536, 409, 1184, 999, 700, nan, 1395, 346, 46, nan, 384, 124, 245, 831, 1389, 1341, 491, 885, nan, nan, 1060, 131, 1124, 1305, 455, 16, 601, 1145, 1359, 1093, 426, 1379, 447, 353, 640, 38, 866, 55, 901, 28, 484, 878, 1388, 379, 1614, 1400, 362, 72, 617, 571, 1408, 1026, 510, 442, nan, 611, 931, 563, 1301, 397, 1185, 41, 822, 578, 42, 195, 670, 478, 197, nan, 1279, 1671, 163, 663, 1281, 1343, 1373, 321, 430, 144, 1099, 1213, 137, 1032, 1018, 1376, nan, 1171, 83, 1386, 1229, 1378, 824, nan, 864, 529, 1233, 1531, 1532, 1037, 268, 561, 524, 507, 237, 531, 453, 1186, 705, 309, 1549, 671, 269, 940, 828, 465, 97, nan, 1474, 1095, nan, 214, 562, 632, 658, 656, 1189, 412, nan, 795, 327, 1615, nan, nan, 840, 652, 1268, 514, 459, 1097, nan, 1200, 224, 182, 278, nan, 1493, 907, 548, nan, 1402, 1068, 1272, 84, 434, 729, 818, 1090, 597, 57, 635, 568, 1136, nan, 322, 1410, nan, 1551, 82, 972, nan, nan, 1629, 609, 633, 490, 845, 698, 842, 522, 141, 542, 718, 1355, 686, 614, 324, 811, 1332, 1263, 420, 799, 1127, 1003, 1010, 575, 1646, 376, 43, 250, nan, 236, nan, 341, 1104, 206, 289, 1441, 451, 391, 1288, 505, 4, 549, nan, 525, 1028, 111, 295, nan, 1225, 424, 1653, 243, 1526, 1284, 1424, 1072, 238, 1031, nan, 1223, 210, 303, 1017, 655, nan, 715, 1631, 64, 1191, 1244, 1231, 1130, 30, 494, 121, 703, 429, 1022, 1173, 466, 1346, 492, 1494, 599, 1422, nan, 29, 21, 951, 1240, 604, 14, 508, nan, 973, 314, 909, 1298, 1655, 897, 375, 1057, 1079, 1433, 1055, nan, 1162, 1296, 541, 65, 651, 1014, 1308, 589, 410, 613, 892, 1210, 383, 432, 631, 602, 77, 1636, 774, 767, 835, 711, 689, 1309, nan, 749, 263, 546, 1110, nan, 934, nan, 36, 122, 806, 1067, 7, 1248, 1357, 481, 743, 1350, 1639, 964, 304, nan, 1224, 787, 1406, 390, 688, 1062, 560, 808, 1077, nan, 1675, 539, nan, nan, nan, 1265, 105, nan, 401, 56, 970, 310, 564, 1345, 647, 1663, 387, 1038, 441, 1352, 654, 68, 109, 1338, 1466, 183, 1016, 318, 1238, 34, 1507, 93, 1241, nan, 403, 138, 74, 518, nan, 1203, 880, 1477, 1076, 535, 991, 803, 249, 1071, 636, 1616, 1431, 1439, 978, 554, 506, 657, 223, 27, 1242, 296, 1021, 445, nan, nan, 1651, 284, 920, nan, 150, 1051, 345, 801, nan, 528, 513, 1362, 152, 623, 1542, nan, nan, 693, 285, 205, 915, 1015, 708, 572, 340, 1215, 867, nan, 485, nan, 662, 1425, 23, 114, 186, 980, 227, 1465, nan, 216, 265, 62, 1228, 1046, 386, 1365, 242, 717, 365, 933, 603, 1054, 1705, 311, 679, 1181, 50, 974, 596, 643, nan, 752, 699, 254, 630, 463, 363, 1574, 1383, 1486, nan, 1418, 889, 1283, 1517, 1190, 464, 502, 1525, 1065, 10, 81, 1562, 395, 1681, nan, 832, 852, 765, 1322, 54, 212, 1048, 1403, 798, 116, 1529, 1291, nan, 1472, 1511, 1042, 1230, 1029, nan, 423, 215, 99, 1039, 1409, nan, 192, 702, 1108, 1396, 1050, 461, nan, 1314, 1002, 638, 349, 775, 204, 1556, 2, nan, 313, 1412, nan, 672, 900, 1640, 582, 155, 128, 1385, 389, 1342, 140, 1125, 1660, 48, 1445, 40, 433, 1227, 1082, nan, 167, 1307, 517, 1482, 735, 1500, 1320, 971, 1478, 741, 1109, 519, 361, 151, 19, 1372, 425, 301, 351, 1259, 1143, 584, 191, 594, 753, 947, 579, 607, 1669, 919, 1334, 626, 1668, 291, 1027, 271, 328, 462, 176, 279, 1092, 893, 58, 727, 942, 846, 1035, nan, 96, 615, 300, 1484, 860, 1247, 11, nan, 1100, 720, 1178, 1234, 1161, 1187, 1547, 149, 1545, 1670, 145, 45, 1382, 355, 342, 1237, 1179, 965, 891, 211, 1641, 1654, 498, 745, 1121, 732, 1624, 797, nan, 1174, nan, 168, 101, 1688, 1488, 1083, 1321, nan, 911, 858, nan, 551, nan, 1129, nan, nan, 1327, nan, 762, 719, nan, 1297, 924, 1630, 1696, nan, nan, 1693, 1303, 830, 768, 1116, 1089, 338, 286, 534, 511, 1508, 1034, 722, 1004, 398, 850, 1368, 1515, 1045, 469, 1325, 761, 1609, 1520, 697, 844, 378, 1236, 331, 975, 411, 692, 957, 1180, 744, nan, 1375, 132, 344, 372, 1381, 862, 1292, 756, nan, 591, nan, 1114, 1005, 1499, 427, 1643, 1581, 723, 833, 1447, 476, 1135, 154, nan, nan, 1656, 173, 222, nan, 1394, 475, 1349, nan, 769, 73, 1680, 956, 1158, 1637, 17, 941, 218, 829, 493, 169, 1674, 1509, nan, nan, 117, 953, 849, nan, nan, nan, 115, 1538, 984, 1450, 1066, 784, 1257, 1464, 547, 305, 1175, 1324, 419, 1267, 649, nan, 1679, 754, nan, 545, 716, 1047, nan, nan, 1192, 479, 1420, nan, 113, 130, nan, nan, 1519, 994, 348, 104, 1141, 1290, nan, nan, 1177, nan, 123, 1132, 1044, 1502, 1326, 1052, 990, 1294, 1221, 1489, 1339, 333, nan, 792, 87, 380, nan, 1626, 1353, 1337, 1159, 710, 1621, 938, 1678, 1235, 966, 184, 1458, 1440, 1446, 75, 1252, nan, 107, 1591, 1521, 917, 782, 1087, 1209, 780, nan, 37, 1059, nan, 969, 174, 1061, 1404, nan, 1596, 1415, nan, 1103, nan, 1360, 1311, 1451, 1576, 24, nan, nan, 821, 421, 962, 1289, 489, 1113, 1604, 997, 1479, 1510, 1149, 945, 1552, 1473, 1652, 1105, 1587, 1053, 1102, 306, 874, 1364, nan, 1423, 809, 1483, 1000, 1214, 1398, 1347, 1393, 1468, 25, 133, 665, nan, 393, 1448, 682, 339, 557, 758, 1285, nan, 552, 1043, 1344, nan, 1024, 1650, 1232, nan, 1649, 110, 1041, 1672, 1013, 1457, 879, 1064, 1544, nan, 865, 725, 1535, 921, 1497, 1708, 751, 456, 1008, 800, 1258, 949, 583, nan, 890, 1550, 1211, 1165, 1255, 1262, 15, 1667, 1319, nan, 1595, 802, 788, 1063, 827, 1148, 1498, 734, 134, 1518, 1610, nan, 1249, 1530, nan, nan, 895, 1188, 728, 1315, 240], 'MORGAN_Features': 0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...\n",
      "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "6       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "                              ...                        \n",
      "9640    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9751    [0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9760    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9787    [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9801    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: Morgan_Features, Length: 1478, dtype: object}\n"
     ]
    }
   ],
   "source": [
    "with open('VEC_drug_feats_morgan.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "print(x.keys())\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target feature extracting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prot-bert-bfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "original_path = sys.path.copy()\n",
    "sys.path.insert(0, '../../nodefeaturing')\n",
    "import generate_entity_embedding\n",
    "importlib.reload(generate_entity_embedding)\n",
    "sys.path = original_path\n",
    "from generate_entity_embedding import generating_pro_feature, generating_drug_feature\n",
    "\n",
    "df = pd.read_csv(\"origin_proteins.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "df.columns = [\"Target_ID\", \"Target\"]\n",
    "# Select unique rows based on the 'Drug' column\n",
    "unique_targets_df = df.drop_duplicates(subset=['Target'])\n",
    "print(len(unique_targets_df))\n",
    "\n",
    "pro_df = generating_pro_feature(unique_targets_df)\n",
    "# Print the resulting DataFrame\n",
    "# print(len(pro_df))\n",
    "# print(pro_df)\n",
    "tmp = pro_df[['Target_ID', 'Target','PROT_BERT_BFD_Features']]\n",
    "print(tmp.head())\n",
    "\n",
    "target_id_array = tmp['Target_ID'].values\n",
    "target_array = tmp['Target'].values\n",
    "target_feat = tmp['PROT_BERT_BFD_Features']\n",
    "\n",
    "target_feat = {'Target_ID': target_id_array, 'Target': target_array, 'PROT_BERT_BFD_Features': target_feat}\n",
    "\n",
    "\n",
    "''' change Protein ID (UniProtKB) to Gene Id (NCBI gene (formerly Entrezgene) ID) '''\n",
    "a = pd.read_csv(\"convert_table\", delimiter= \"\\t\")\n",
    "new = []\n",
    "new_emb = []\n",
    "for x_idx, pro in enumerate(target_feat['Target_ID']) :\n",
    "    for a_idx, name in enumerate(a[\"UniProtKB Gene Name ID\"]) : \n",
    "        if pro == name :\n",
    "            new.append(a['NCBI gene (formerly Entrezgene) ID'].iloc[a_idx])\n",
    "            new_emb.append(target_feat[\"PROT_BERT_BFD_Features\"][x_idx])\n",
    "            break\n",
    "\n",
    "new_x = {}\n",
    "new_x[\"Target_ID\"] = new\n",
    "new_x[\"PROT_BERT_BFD_Features\"]= new_emb\n",
    "new_x = pd.DataFrame(new_x)\n",
    "print(new_x)\n",
    "\n",
    "''' Convert Gene ID to Encoding number And Build Mapping array for training '''\n",
    "with open('entity_drug.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "enco = []\n",
    "for i in new :\n",
    "    if i in data.keys() :\n",
    "        enco.append(data[i])\n",
    "    else : \n",
    "        enco.append(np.NaN)\n",
    "#print(enco)\n",
    "new_x[\"Gene_enco\"] = enco\n",
    "new_x = new_x.sort_values(by = \"Gene_enco\").reset_index(drop=True)\n",
    "new_x[\"mapping_arr\"] = range(0,len(new_x))\n",
    "vec_pfeat = {\"Target_ID\" : new_x[\"Target_ID\"].values, \"PROT_BERT_BFD_Features\" : new_x[\"PROT_BERT_BFD_Features\"], \"Gene_enco\" : new_x[\"Gene_enco\"].values, \"map_arr\" : new_x[\"mapping_arr\"].values}\n",
    "\n",
    "with open(\"VEC_target_feats_prot_bert_bfd.pkl\", 'wb') as f:\n",
    "    pickle.dump(vec_pfeat, f)\n",
    "print(f\"Dictionary saved as pickle file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "1190\n",
      "(1024,)\n",
      "<class 'numpy.ndarray'>\n",
      "{'Target_ID': array(['Gene::3725', 'Gene::10014', 'Gene::2099', ..., 'Gene::107080644',\n",
      "       'Gene::', 'Gene::104909134'], dtype=object), 'PROT_BERT_BFD_Features': 0       [0.07999994, 0.07057223, -0.005482291, -0.1017...\n",
      "1       [0.09861873, 0.03345179, -0.0198907, 0.0481128...\n",
      "2       [0.0921928, 0.0055588936, 0.045266334, 0.19758...\n",
      "3       [0.065954186, 0.028363759, 0.0691558, 0.004959...\n",
      "4       [-0.0011728306, 0.07787305, 0.04118555, 0.1335...\n",
      "                              ...                        \n",
      "1380    [0.08340913, 0.024366094, 0.0077468874, 0.0336...\n",
      "1381    [0.10049024, 0.09608592, -0.09092753, -0.11782...\n",
      "1382    [0.05332482, 0.0827113, -0.0252028, -0.0764334...\n",
      "1383    [-0.005920995, -0.071281135, 0.12536465, -0.17...\n",
      "1384    [0.037600864, 0.09319527, 0.11077085, -0.01084...\n",
      "Name: PROT_BERT_BFD_Features, Length: 1385, dtype: object, 'Gene_enco': array([1716., 1718., 1720., ...,   nan,   nan,   nan]), 'map_arr': array([   0,    1,    2, ..., 1382, 1383, 1384])}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "with open('VEC_target_feats_prot_bert_bfd.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "print(type(x['PROT_BERT_BFD_Features']))\n",
    "random_integer = random.randint(0, 1384); print(random_integer)\n",
    "print(x['PROT_BERT_BFD_Features'][random_integer].shape) # (1024, ) \n",
    "print(type(x['PROT_BERT_BFD_Features'][random_integer]))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prostt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "original_path = sys.path.copy()\n",
    "sys.path.insert(0, '../../nodefeaturing')\n",
    "import generate_entity_embedding\n",
    "importlib.reload(generate_entity_embedding)\n",
    "sys.path = original_path\n",
    "from generate_entity_embedding import generating_pro_feature, generating_drug_feature\n",
    "\n",
    "df = pd.read_csv(\"origin_proteins.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "df.columns = [\"Target_ID\", \"Target\"]\n",
    "# Select unique rows based on the 'Drug' column\n",
    "unique_targets_df = df.drop_duplicates(subset=['Target'])\n",
    "print(len(unique_targets_df))\n",
    "\n",
    "pro_df = generating_pro_feature(unique_targets_df)\n",
    "# Print the resulting DataFrame\n",
    "# print(len(pro_df))\n",
    "# print(pro_df)\n",
    "tmp = pro_df[['Target_ID', 'Target','PROSTT5_Features']]\n",
    "print(tmp.head())\n",
    "\n",
    "target_id_array = tmp['Target_ID'].values\n",
    "target_array = tmp['Target'].values\n",
    "target_feat = tmp['PROSTT5_Features']\n",
    "\n",
    "target_feat = {'Target_ID': target_id_array, 'Target': target_array, 'PROSTT5_Features': target_feat}\n",
    "\n",
    "\n",
    "''' change Protein ID (UniProtKB) to Gene Id (NCBI gene (formerly Entrezgene) ID) '''\n",
    "a = pd.read_csv(\"convert_table\", delimiter= \"\\t\")\n",
    "new = []\n",
    "new_emb = []\n",
    "for x_idx, pro in enumerate(target_feat['Target_ID']) :\n",
    "    for a_idx, name in enumerate(a[\"UniProtKB Gene Name ID\"]) : \n",
    "        if pro == name :\n",
    "            new.append(a['NCBI gene (formerly Entrezgene) ID'].iloc[a_idx])\n",
    "            new_emb.append(target_feat[\"PROSTT5_Features\"][x_idx])\n",
    "            break\n",
    "\n",
    "new_x = {}\n",
    "new_x[\"Target_ID\"] = new\n",
    "new_x[\"PROSTT5_Features\"]= new_emb\n",
    "new_x = pd.DataFrame(new_x)\n",
    "print(new_x)\n",
    "\n",
    "''' Convert Gene ID to Encoding number And Build Mapping array for training '''\n",
    "with open('entity_drug.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "enco = []\n",
    "for i in new :\n",
    "    if i in data.keys() :\n",
    "        enco.append(data[i])\n",
    "    else : \n",
    "        enco.append(np.NaN)\n",
    "#print(enco)\n",
    "new_x[\"Gene_enco\"] = enco\n",
    "new_x = new_x.sort_values(by = \"Gene_enco\").reset_index(drop=True)\n",
    "new_x[\"mapping_arr\"] = range(0,len(new_x))\n",
    "vec_pfeat = {\"Target_ID\" : new_x[\"Target_ID\"].values, \"PROSTT5_Features\" : new_x[\"PROSTT5_Features\"], \"Gene_enco\" : new_x[\"Gene_enco\"].values, \"map_arr\" : new_x[\"mapping_arr\"].values}\n",
    "\n",
    "with open(\"VEC_target_feats_prostt5.pkl\", 'wb') as f:\n",
    "    pickle.dump(vec_pfeat, f)\n",
    "print(f\"Dictionary saved as pickle file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,)\n",
      "{'Target_ID': array(['Gene::3725', 'Gene::10014', 'Gene::2099', ..., 'Gene::107080644',\n",
      "       'Gene::', 'Gene::104909134'], dtype=object), 'PROSTT5_Features': 0       [0.29774928, -0.10099035, -0.21374746, -0.0479...\n",
      "1       [0.27234846, -0.07167395, -0.1250989, -0.00307...\n",
      "2       [0.08530362, -0.20161305, -0.24574412, -0.0009...\n",
      "3       [0.17284732, -0.074036844, -0.53658897, -0.079...\n",
      "4       [0.05752742, -0.18257628, 0.040561356, 0.24134...\n",
      "                              ...                        \n",
      "1380    [0.18186754, -0.09916633, -0.32874444, 0.42915...\n",
      "1381    [0.25534803, 0.14017427, 0.37623963, 0.2106000...\n",
      "1382    [0.16089754, -0.12235806, 0.090396516, 0.21785...\n",
      "1383    [0.16730103, -0.2704303, -0.025364595, 0.08650...\n",
      "1384    [0.15681213, -0.20944996, -0.10326583, -0.2648...\n",
      "Name: PROSTT5_Features, Length: 1385, dtype: object, 'Gene_enco': array([1716., 1718., 1720., ...,   nan,   nan,   nan]), 'map_arr': array([   0,    1,    2, ..., 1382, 1383, 1384])}\n"
     ]
    }
   ],
   "source": [
    "with open('VEC_target_feats_prostt5.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "print(x['PROSTT5_Features'][1300].shape) # (1024, ) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prot_t5_xl_bfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "original_path = sys.path.copy()\n",
    "sys.path.insert(0, '../../nodefeaturing')\n",
    "import generate_entity_embedding\n",
    "importlib.reload(generate_entity_embedding)\n",
    "sys.path = original_path\n",
    "from generate_entity_embedding import generating_pro_feature, generating_drug_feature\n",
    "\n",
    "df = pd.read_csv(\"origin_proteins.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "df.columns = [\"Target_ID\", \"Target\"]\n",
    "# Select unique rows based on the 'Drug' column\n",
    "unique_targets_df = df.drop_duplicates(subset=['Target'])\n",
    "print(len(unique_targets_df))\n",
    "\n",
    "pro_df = generating_pro_feature(unique_targets_df)\n",
    "# Print the resulting DataFrame\n",
    "# print(len(pro_df))\n",
    "# print(pro_df)\n",
    "tmp = pro_df[['Target_ID', 'Target','PROT_T5_XL_BFD_Features']]\n",
    "print(tmp.head())\n",
    "\n",
    "target_id_array = tmp['Target_ID'].values\n",
    "target_array = tmp['Target'].values\n",
    "target_feat = tmp['PROT_T5_XL_BFD_Features']\n",
    "\n",
    "target_feat = {'Target_ID': target_id_array, 'Target': target_array, 'PROT_T5_XL_BFD_Features': target_feat}\n",
    "\n",
    "\n",
    "''' change Protein ID (UniProtKB) to Gene Id (NCBI gene (formerly Entrezgene) ID) '''\n",
    "a = pd.read_csv(\"convert_table\", delimiter= \"\\t\")\n",
    "new = []\n",
    "new_emb = []\n",
    "for x_idx, pro in enumerate(target_feat['Target_ID']) :\n",
    "    for a_idx, name in enumerate(a[\"UniProtKB Gene Name ID\"]) : \n",
    "        if pro == name :\n",
    "            new.append(a['NCBI gene (formerly Entrezgene) ID'].iloc[a_idx])\n",
    "            new_emb.append(target_feat[\"PROT_T5_XL_BFD_Features\"][x_idx])\n",
    "            break\n",
    "\n",
    "new_x = {}\n",
    "new_x[\"Target_ID\"] = new\n",
    "new_x[\"PROT_T5_XL_BFD_Features\"]= new_emb\n",
    "new_x = pd.DataFrame(new_x)\n",
    "print(new_x)\n",
    "\n",
    "''' Convert Gene ID to Encoding number And Build Mapping array for training '''\n",
    "with open('entity_drug.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "enco = []\n",
    "for i in new :\n",
    "    if i in data.keys() :\n",
    "        enco.append(data[i])\n",
    "    else : \n",
    "        enco.append(np.NaN)\n",
    "#print(enco)\n",
    "new_x[\"Gene_enco\"] = enco\n",
    "new_x = new_x.sort_values(by = \"Gene_enco\").reset_index(drop=True)\n",
    "new_x[\"mapping_arr\"] = range(0,len(new_x))\n",
    "vec_pfeat = {\"Target_ID\" : new_x[\"Target_ID\"].values, \"PROT_T5_XL_BFD_Features\" : new_x[\"PROT_T5_XL_BFD_Features\"], \"Gene_enco\" : new_x[\"Gene_enco\"].values, \"map_arr\" : new_x[\"mapping_arr\"].values}\n",
    "\n",
    "with open(\"VEC_target_feats_prot_t5_xl_bfd.pkl\", 'wb') as f:\n",
    "    pickle.dump(vec_pfeat, f)\n",
    "print(f\"Dictionary saved as pickle file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n",
      "(1024,)\n",
      "{'Target_ID': array(['Gene::3725', 'Gene::10014', 'Gene::2099', ..., 'Gene::107080644',\n",
      "       'Gene::', 'Gene::104909134'], dtype=object), 'PROT_T5_XL_BFD_Features': 0       [0.3061483, -0.479783, -0.41335437, 0.57882804...\n",
      "1       [0.36122397, -0.28566578, 0.2765294, 0.2626283...\n",
      "2       [0.059911527, -0.3247478, -0.076610416, 0.1895...\n",
      "3       [0.22034469, -0.08428178, -0.37653744, -0.1772...\n",
      "4       [-0.15420882, -0.11498539, -0.5919012, -0.2620...\n",
      "                              ...                        \n",
      "1380    [0.2510656, 0.036168, -0.11522516, 0.31929836,...\n",
      "1381    [0.44312462, -0.26340228, 0.25449044, -0.22715...\n",
      "1382    [0.20614415, -0.0829238, 0.030516902, 0.291382...\n",
      "1383    [-0.19570357, -0.34753188, 0.06822972, -0.0065...\n",
      "1384    [0.22904179, -0.19650827, -0.10441386, 0.12113...\n",
      "Name: PROT_T5_XL_BFD_Features, Length: 1385, dtype: object, 'Gene_enco': array([1716., 1718., 1720., ...,   nan,   nan,   nan]), 'map_arr': array([   0,    1,    2, ..., 1382, 1383, 1384])}\n"
     ]
    }
   ],
   "source": [
    "with open('VEC_target_feats_prot_t5_xl_bfd.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "import random\n",
    "random_integer = random.randint(0, 1384); print(random_integer)\n",
    "print(x['PROT_T5_XL_BFD_Features'][random_integer].shape) # (1024, ) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prot-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "original_path = sys.path.copy()\n",
    "sys.path.insert(0, '../../nodefeaturing')\n",
    "import generate_entity_embedding\n",
    "importlib.reload(generate_entity_embedding)\n",
    "sys.path = original_path\n",
    "from generate_entity_embedding import generating_pro_feature, generating_drug_feature\n",
    "\n",
    "df = pd.read_csv(\"origin_proteins.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "df.columns = [\"Target_ID\", \"Target\"]\n",
    "# Select unique rows based on the 'Drug' column\n",
    "unique_targets_df = df.drop_duplicates(subset=['Target'])\n",
    "print(len(unique_targets_df))\n",
    "\n",
    "pro_df = generating_pro_feature(unique_targets_df)\n",
    "# Print the resulting DataFrame\n",
    "# print(len(pro_df))\n",
    "# print(pro_df)\n",
    "tmp = pro_df[['Target_ID', 'Target','PROT_BERT_Features']]\n",
    "print(tmp.head())\n",
    "\n",
    "target_id_array = tmp['Target_ID'].values\n",
    "target_array = tmp['Target'].values\n",
    "target_feat = tmp['PROT_BERT_Features']\n",
    "\n",
    "target_feat = {'Target_ID': target_id_array, 'Target': target_array, 'PROT_BERT_Features': target_feat}\n",
    "\n",
    "\n",
    "''' change Protein ID (UniProtKB) to Gene Id (NCBI gene (formerly Entrezgene) ID) '''\n",
    "a = pd.read_csv(\"convert_table\", delimiter= \"\\t\")\n",
    "new = []\n",
    "new_emb = []\n",
    "for x_idx, pro in enumerate(target_feat['Target_ID']) :\n",
    "    for a_idx, name in enumerate(a[\"UniProtKB Gene Name ID\"]) : \n",
    "        if pro == name :\n",
    "            new.append(a['NCBI gene (formerly Entrezgene) ID'].iloc[a_idx])\n",
    "            new_emb.append(target_feat[\"PROT_BERT_Features\"][x_idx])\n",
    "            break\n",
    "\n",
    "new_x = {}\n",
    "new_x[\"Target_ID\"] = new\n",
    "new_x[\"PROT_BERT_Features\"]= new_emb\n",
    "new_x = pd.DataFrame(new_x)\n",
    "print(new_x)\n",
    "\n",
    "''' Convert Gene ID to Encoding number And Build Mapping array for training '''\n",
    "with open('entity_drug.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "enco = []\n",
    "for i in new :\n",
    "    if i in data.keys() :\n",
    "        enco.append(data[i])\n",
    "    else : \n",
    "        enco.append(np.NaN)\n",
    "#print(enco)\n",
    "new_x[\"Gene_enco\"] = enco\n",
    "new_x = new_x.sort_values(by = \"Gene_enco\").reset_index(drop=True)\n",
    "new_x[\"mapping_arr\"] = range(0,len(new_x))\n",
    "vec_pfeat = {\"Target_ID\" : new_x[\"Target_ID\"].values, \"PROT_BERT_Features\" : new_x[\"PROT_BERT_Features\"], \"Gene_enco\" : new_x[\"Gene_enco\"].values, \"map_arr\" : new_x[\"mapping_arr\"].values}\n",
    "\n",
    "with open(\"VEC_target_feats_prot_bert.pkl\", 'wb') as f:\n",
    "    pickle.dump(vec_pfeat, f)\n",
    "print(f\"Dictionary saved as pickle file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "971\n",
      "(1024,)\n",
      "{'Target_ID': array(['Gene::3725', 'Gene::10014', 'Gene::2099', ..., 'Gene::107080644',\n",
      "       'Gene::', 'Gene::104909134'], dtype=object), 'PROT_BERT_Features': 0       [0.055428114, -0.05638484, -0.14977953, -0.084...\n",
      "1       [0.1306466, 0.00059073814, -0.12146838, 0.0223...\n",
      "2       [-0.02885853, 0.119123675, -0.11759645, -0.163...\n",
      "3       [0.14950863, 0.13119787, -0.16628464, -0.27110...\n",
      "4       [0.0337875, 0.06081189, 0.02806383, -0.0159795...\n",
      "                              ...                        \n",
      "1380    [0.11205415, 0.057612926, -0.08516071, -0.2570...\n",
      "1381    [0.024919827, 0.085816376, -0.027137723, -0.14...\n",
      "1382    [0.11519282, 0.051940884, 0.08040943, -0.06652...\n",
      "1383    [0.08724613, 0.07569128, -0.053993437, -0.0662...\n",
      "1384    [0.110392936, 0.07712469, -0.05605019, -0.0872...\n",
      "Name: PROT_BERT_Features, Length: 1385, dtype: object, 'Gene_enco': array([1716., 1718., 1720., ...,   nan,   nan,   nan]), 'map_arr': array([   0,    1,    2, ..., 1382, 1383, 1384])}\n"
     ]
    }
   ],
   "source": [
    "with open('VEC_target_feats_prot_bert.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "import random\n",
    "random_integer = random.randint(0, 1384); print(random_integer)\n",
    "print(x['PROT_BERT_Features'][random_integer].shape) # (1024, ) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prot_t5_xl_uniref50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "original_path = sys.path.copy()\n",
    "sys.path.insert(0, '../../nodefeaturing')\n",
    "import generate_entity_embedding\n",
    "importlib.reload(generate_entity_embedding)\n",
    "sys.path = original_path\n",
    "from generate_entity_embedding import generating_pro_feature, generating_drug_feature\n",
    "\n",
    "df = pd.read_csv(\"origin_proteins.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "df.columns = [\"Target_ID\", \"Target\"]\n",
    "# Select unique rows based on the 'Drug' column\n",
    "unique_targets_df = df.drop_duplicates(subset=['Target'])\n",
    "print(len(unique_targets_df))\n",
    "\n",
    "pro_df = generating_pro_feature(unique_targets_df)\n",
    "# Print the resulting DataFrame\n",
    "# print(len(pro_df))\n",
    "# print(pro_df)\n",
    "tmp = pro_df[['Target_ID', 'Target','PROT_T5_XL_UNIREF50_Features']]\n",
    "print(tmp.head())\n",
    "\n",
    "target_id_array = tmp['Target_ID'].values\n",
    "target_array = tmp['Target'].values\n",
    "target_feat = tmp['PROT_T5_XL_UNIREF50_Features']\n",
    "\n",
    "target_feat = {'Target_ID': target_id_array, 'Target': target_array, 'PROT_T5_XL_UNIREF50_Features': target_feat}\n",
    "\n",
    "\n",
    "''' change Protein ID (UniProtKB) to Gene Id (NCBI gene (formerly Entrezgene) ID) '''\n",
    "a = pd.read_csv(\"convert_table\", delimiter= \"\\t\")\n",
    "new = []\n",
    "new_emb = []\n",
    "for x_idx, pro in enumerate(target_feat['Target_ID']) :\n",
    "    for a_idx, name in enumerate(a[\"UniProtKB Gene Name ID\"]) : \n",
    "        if pro == name :\n",
    "            new.append(a['NCBI gene (formerly Entrezgene) ID'].iloc[a_idx])\n",
    "            new_emb.append(target_feat[\"PROT_T5_XL_UNIREF50_Features\"][x_idx])\n",
    "            break\n",
    "\n",
    "new_x = {}\n",
    "new_x[\"Target_ID\"] = new\n",
    "new_x[\"PROT_T5_XL_UNIREF50_Features\"]= new_emb\n",
    "new_x = pd.DataFrame(new_x)\n",
    "print(new_x)\n",
    "\n",
    "''' Convert Gene ID to Encoding number And Build Mapping array for training '''\n",
    "with open('entity_drug.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "enco = []\n",
    "for i in new :\n",
    "    if i in data.keys() :\n",
    "        enco.append(data[i])\n",
    "    else : \n",
    "        enco.append(np.NaN)\n",
    "#print(enco)\n",
    "new_x[\"Gene_enco\"] = enco\n",
    "new_x = new_x.sort_values(by = \"Gene_enco\").reset_index(drop=True)\n",
    "new_x[\"mapping_arr\"] = range(0,len(new_x))\n",
    "vec_pfeat = {\"Target_ID\" : new_x[\"Target_ID\"].values, \"PROT_T5_XL_UNIREF50_Features\" : new_x[\"PROT_T5_XL_UNIREF50_Features\"], \"Gene_enco\" : new_x[\"Gene_enco\"].values, \"map_arr\" : new_x[\"mapping_arr\"].values}\n",
    "\n",
    "with open(\"VEC_target_feats_prot_t5_xl_uniref50.pkl\", 'wb') as f:\n",
    "    pickle.dump(vec_pfeat, f)\n",
    "print(f\"Dictionary saved as pickle file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1320\n",
      "(1024,)\n",
      "{'Target_ID': array(['Gene::3725', 'Gene::10014', 'Gene::2099', ..., 'Gene::107080644',\n",
      "       'Gene::', 'Gene::104909134'], dtype=object), 'PROT_T5_XL_UNIREF50_Features': 0       [0.23339663, -0.16107665, -0.2483075, 0.454003...\n",
      "1       [0.19799815, -0.17363514, -0.000113108945, 0.2...\n",
      "2       [-0.21022151, -0.20351231, -0.03177288, 0.1806...\n",
      "3       [0.16953245, -0.12390078, 0.11324732, 0.272348...\n",
      "4       [0.12069299, -0.22523174, -0.17325203, 0.00566...\n",
      "                              ...                        \n",
      "1380    [0.1424455, -0.11220649, 0.0985885, 0.03786475...\n",
      "1381    [0.16387756, -0.43449685, -0.012802006, -0.075...\n",
      "1382    [0.23513082, -0.1760802, 0.14090289, 0.2739504...\n",
      "1383    [0.095515124, -0.4050157, -0.016741376, 0.0647...\n",
      "1384    [0.12722236, -0.07694693, 0.09211067, 0.219167...\n",
      "Name: PROT_T5_XL_UNIREF50_Features, Length: 1385, dtype: object, 'Gene_enco': array([1716., 1718., 1720., ...,   nan,   nan,   nan]), 'map_arr': array([   0,    1,    2, ..., 1382, 1383, 1384])}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "with open('VEC_target_feats_prot_t5_xl_uniref50.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "random_integer = random.randint(0, 1384); print(random_integer)\n",
    "print(x['PROT_T5_XL_UNIREF50_Features'][random_integer].shape) # (1024, ) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n",
      "(1024,)\n",
      "{'Target_ID': array(['Gene::1586', 'Gene::91', 'Gene::5291', ..., 'Gene::3782',\n",
      "       'Gene::8510', 'Gene::6330'], dtype=object), 'ProtBERT_Features': 0       [0.10895072, 0.07753971, 0.14122498, -0.004359...\n",
      "1       [0.07643488, 0.046613887, 0.11374605, 0.034627...\n",
      "2       [0.079430476, 0.028320394, 0.066061325, 0.0013...\n",
      "3       [0.13558188, 0.013845695, 0.12252106, 0.037885...\n",
      "4       [0.11597116, 0.016289648, 0.10930617, 0.023096...\n",
      "                              ...                        \n",
      "1380    [0.026317561, 0.002467662, 0.088176705, 0.0322...\n",
      "1381    [0.025081605, 0.011302358, 0.13916573, 0.03344...\n",
      "1382    [0.06859535, -0.012777425, 0.09564611, 0.01458...\n",
      "1383    [0.05675855, -0.017336804, 0.099945456, 0.0534...\n",
      "1384    [0.08563626, -0.021550683, 0.13873681, 0.03547...\n",
      "Name: ProtBERT_Features, Length: 1385, dtype: object, 'Gene_enco': array([1716., 1718., 1720., ...,   nan,   nan,   nan]), 'map_arr': array([   0,    1,    2, ..., 1382, 1383, 1384])}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "with open('VEC_target_feats_tmp.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "random_integer = random.randint(0, 1384); print(random_integer)\n",
    "print(x['ProtBERT_Features'][random_integer].shape) # (1024, ) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prot_t5_xxl_uniref50 -> OOM Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import pandas as pd\n",
    "original_path = sys.path.copy()\n",
    "sys.path.insert(0, '../../nodefeaturing')\n",
    "import gen_feat_prot_t5_xxl_uniref50\n",
    "# import generating_feature\n",
    "# importlib.reload(generating_feature)\n",
    "importlib.reload(gen_feat_prot_t5_xxl_uniref50)\n",
    "sys.path = original_path\n",
    "# from generating_feature import generating_pro_feature, generating_drug_feature\n",
    "from gen_feat_prot_t5_xxl_uniref50 import generating_pro_feature, generating_drug_feature\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"origin_proteins.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "df.columns = [\"Target_ID\", \"Target\"]\n",
    "# Select unique rows based on the 'Drug' column\n",
    "unique_targets_df = df.drop_duplicates(subset=['Target'])\n",
    "print(len(unique_targets_df))\n",
    "\n",
    "pro_df = generating_pro_feature(unique_targets_df)\n",
    "# Print the resulting DataFrame\n",
    "# print(len(pro_df))\n",
    "# print(pro_df)\n",
    "tmp = pro_df[['Target_ID', 'Target','ProtBERT_Features']]\n",
    "print(tmp.head())\n",
    "\n",
    "target_id_array = tmp['Target_ID'].values\n",
    "target_array = tmp['Target'].values\n",
    "target_feat = tmp['ProtBERT_Features']\n",
    "\n",
    "target_feat = {'Target_ID': target_id_array, 'Target': target_array, 'ProtBERT_Features': target_feat}\n",
    "\n",
    "\n",
    "''' change Protein ID (UniProtKB) to Gene Id (NCBI gene (formerly Entrezgene) ID) '''\n",
    "a = pd.read_csv(\"convert_table\", delimiter= \"\\t\")\n",
    "new = []\n",
    "new_emb = []\n",
    "for x_idx, pro in enumerate(target_feat['Target_ID']) :\n",
    "    for a_idx, name in enumerate(a[\"UniProtKB Gene Name ID\"]) : \n",
    "        if pro == name :\n",
    "            new.append(a['NCBI gene (formerly Entrezgene) ID'].iloc[a_idx])\n",
    "            new_emb.append(target_feat[\"ProtBERT_Features\"][x_idx])\n",
    "            break\n",
    "\n",
    "new_x = {}\n",
    "new_x[\"Target_ID\"] = new\n",
    "new_x[\"ProtBERT_Features\"]= new_emb\n",
    "new_x = pd.DataFrame(new_x)\n",
    "print(new_x)\n",
    "\n",
    "''' Convert Gene ID to Encoding number And Build Mapping array for training '''\n",
    "with open('entity_drug.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "enco = []\n",
    "for i in new :\n",
    "    if i in data.keys() :\n",
    "        enco.append(data[i])\n",
    "    else : \n",
    "        enco.append(np.NaN)\n",
    "#print(enco)\n",
    "new_x[\"Gene_enco\"] = enco\n",
    "new_x = new_x.sort_values(by = \"Gene_enco\").reset_index(drop=True)\n",
    "new_x[\"mapping_arr\"] = range(0,len(new_x))\n",
    "vec_pfeat = {\"Target_ID\" : new_x[\"Target_ID\"].values, \"ProtBERT_Features\" : new_x[\"ProtBERT_Features\"], \"Gene_enco\" : new_x[\"Gene_enco\"].values, \"map_arr\" : new_x[\"mapping_arr\"].values}\n",
    "\n",
    "with open(\"VEC_target_feats_prot_t5_xxl_uniref50.pkl\", 'wb') as f:\n",
    "    pickle.dump(vec_pfeat, f)\n",
    "print(f\"Dictionary saved as pickle file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'VEC_target_feats_prot_t5_xxl_uniref50.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mVEC_target_feats_prot_t5_xxl_uniref50.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     x \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(x)\n",
      "File \u001b[0;32m~/.conda/envs/sumgnn/lib/python3.8/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'VEC_target_feats_prot_t5_xxl_uniref50.pkl'"
     ]
    }
   ],
   "source": [
    "with open('VEC_target_feats_prot_t5_xxl_uniref50.pkl', 'rb') as f:\n",
    "    x = pickle.load(f, encoding='utf-8')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## github sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_bfd\", do_lower_case=False )\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_bfd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_Example = [\"A E T C Z A O\", \"S K T Z P\", \"S K T Z P Z A O T C\"]\n",
    "sequences_Example = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_Example]\n",
    "\n",
    "ids = tokenizer.batch_encode_plus(sequences_Example, add_special_tokens=True, padding=True)\n",
    "input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "embedding = embedding.last_hidden_state.cpu().numpy()\n",
    "\n",
    "features = [] \n",
    "for seq_num in range(len(embedding)):\n",
    "    seq_len = (attention_mask[seq_num] == 1).sum()\n",
    "    print(len(attention_mask[seq_num]))\n",
    "    print(seq_len)\n",
    "    seq_emd = embedding[seq_num][:seq_len-1]\n",
    "    features.append(seq_emd)\n",
    "\n",
    "print(features[0].shape)\n",
    "print(features[1].shape)\n",
    "print(features[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [: , 0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "size = torch.Size([1, 1024, 1024])\n",
    "seq_tensor = torch.arange(1, size.numel() + 1).reshape(size)\n",
    "print(seq_tensor.shape)\n",
    "print(seq_tensor)\n",
    "print(seq_tensor[:,0].shape)\n",
    "print(seq_tensor[:,0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: torch.Size([256, 1024])\n",
      "Weight size: torch.Size([16, 1024])\n",
      "Bias size: torch.Size([16])\n",
      "Linear transformation successful.\n",
      "Input size: torch.Size([256, 1024])\n",
      "Output size: torch.Size([256, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create tensors with the given sizes\n",
    "input_size = (256, 1024)\n",
    "weight_size = (16, 1024)\n",
    "bias_size = (16,)\n",
    "\n",
    "input_tensor = torch.randn(*input_size)\n",
    "weight_tensor = torch.randn(*weight_size)\n",
    "bias_tensor = torch.randn(*bias_size)\n",
    "\n",
    "print(\"Input size:\", input_tensor.size())\n",
    "print(\"Weight size:\", weight_tensor.size())\n",
    "print(\"Bias size:\", bias_tensor.size())\n",
    "# Check if dimensions match\n",
    "if input_size[1] == weight_size[1] and weight_size[0] == bias_size[0]:\n",
    "    output_tensor = F.linear(input_tensor, weight_tensor, bias_tensor)\n",
    "    print(\"Linear transformation successful.\")\n",
    "    print(\"Input size:\", input_tensor.size())\n",
    "    print(\"Output size:\", output_tensor.size())\n",
    "else:\n",
    "    print(\"Dimensions do not match for linear transformation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumgnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
